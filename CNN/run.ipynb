{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoAP5Dn3DwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, re, sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import gc\n",
        "import skimage.transform"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYucCZeH_dHH",
        "outputId": "bf5a8f1a-cf77-4d04-bd02-e2e639cccbf3"
      },
      "source": [
        "# test if GPUs are available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiMh4isCHnS",
        "outputId": "f4e2fdb6-3ea4-4de8-defb-606570d62e48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQvxwJFDmYk"
      },
      "source": [
        "# set project root, maybe you need to firstly \n",
        "# add shortcut of CS 766 Project to drive.\n",
        "project_root = './drive/MyDrive/CS 766 Project/Project Coding and Data Files'"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJsefpJ-XjF"
      },
      "source": [
        "# class to initialize CNNs\n",
        "class OriginCNN(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1000, 1000, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(5))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs):\n",
        "    self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-3].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiCn4VzdaV8"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50, InceptionV3, VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "class Res50(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    base_model = InceptionV3(\n",
        "      include_top=False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have n classes\n",
        "    predictions = Dense(2)(x)\n",
        "\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs):\n",
        "    self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-2].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3suc98v-bRj"
      },
      "source": [
        "class DR_resized(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate, size=None):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"04_Only_Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # resize\n",
        "      # gray scale\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 1));\n",
        "        for i in range(all_images.shape[0]):\n",
        "          all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "      # color image\n",
        "      else:\n",
        "        all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 3));\n",
        "        for i in range(all_images.shape[0]):\n",
        "          all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "      all_images = all_images_resize\n",
        "\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"04_Only_Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "      \n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"04_Only_Color_Processed_Xest_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # resize\n",
        "      # gray scale\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 1));\n",
        "        for i in range(all_images.shape[0]):\n",
        "          all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "      # color image\n",
        "      else:\n",
        "        all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 3));\n",
        "        for i in range(all_images.shape[0]):\n",
        "          all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "      all_images = all_images_resize\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"04_Only_Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L57sy6Z02c-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Cifar(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      # if (len(all_images[0].shape) == 2):\n",
        "        # all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "      train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "      all_images = train_images\n",
        "      all_labels = train_labels\n",
        "      \n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 10))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      # if (len(all_images[0].shape) == 2):\n",
        "      #   all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "      train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "      all_images = test_images\n",
        "      all_labels = test_labels\n",
        "\n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 10))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOADsxqM3DwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31464ad6-2451-4a4b-b3fb-bd498226ef7b"
      },
      "source": [
        "# initialize CNN\n",
        "# myModel = OriginCNN()\n",
        "myModel = Res50()\n",
        "myModel.build()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 111, 111, 32) 864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 111, 111, 32) 96          conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 111, 111, 32) 0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_377 (Conv2D)             (None, 109, 109, 32) 9216        activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 109, 109, 32) 96          conv2d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 109, 109, 32) 0           batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_378 (Conv2D)             (None, 109, 109, 64) 18432       activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 109, 109, 64) 192         conv2d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 109, 109, 64) 0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 54, 54, 64)   0           activation_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_379 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 54, 54, 80)   240         conv2d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 54, 54, 80)   0           batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_380 (Conv2D)             (None, 52, 52, 192)  138240      activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 52, 52, 192)  576         conv2d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 52, 52, 192)  0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 25, 25, 192)  0           activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_384 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_384 (BatchN (None, 25, 25, 64)   192         conv2d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_384 (Activation)     (None, 25, 25, 64)   0           batch_normalization_384[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_382 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_385 (Conv2D)             (None, 25, 25, 96)   55296       activation_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_382 (BatchN (None, 25, 25, 48)   144         conv2d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_385 (BatchN (None, 25, 25, 96)   288         conv2d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 25, 25, 48)   0           batch_normalization_382[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_385 (Activation)     (None, 25, 25, 96)   0           batch_normalization_385[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_36 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_381 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_383 (Conv2D)             (None, 25, 25, 64)   76800       activation_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_386 (Conv2D)             (None, 25, 25, 96)   82944       activation_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_387 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_36[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 25, 25, 64)   192         conv2d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_383 (BatchN (None, 25, 25, 64)   192         conv2d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_386 (BatchN (None, 25, 25, 96)   288         conv2d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_387 (BatchN (None, 25, 25, 32)   96          conv2d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 25, 25, 64)   0           batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 25, 25, 64)   0           batch_normalization_383[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_386 (Activation)     (None, 25, 25, 96)   0           batch_normalization_386[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_387 (Activation)     (None, 25, 25, 32)   0           batch_normalization_387[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_381[0][0]             \n",
            "                                                                 activation_383[0][0]             \n",
            "                                                                 activation_386[0][0]             \n",
            "                                                                 activation_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_391 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 25, 25, 64)   192         conv2d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 25, 25, 64)   0           batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_389 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_392 (Conv2D)             (None, 25, 25, 96)   55296       activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 25, 25, 48)   144         conv2d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 25, 25, 96)   288         conv2d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 25, 25, 48)   0           batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 25, 25, 96)   0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_37 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_388 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_390 (Conv2D)             (None, 25, 25, 64)   76800       activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_393 (Conv2D)             (None, 25, 25, 96)   82944       activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_394 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_37[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_388 (BatchN (None, 25, 25, 64)   192         conv2d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 25, 25, 64)   192         conv2d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 25, 25, 96)   288         conv2d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 25, 25, 64)   192         conv2d_394[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_388 (Activation)     (None, 25, 25, 64)   0           batch_normalization_388[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 25, 25, 64)   0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 25, 25, 96)   0           batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 25, 25, 64)   0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_388[0][0]             \n",
            "                                                                 activation_390[0][0]             \n",
            "                                                                 activation_393[0][0]             \n",
            "                                                                 activation_394[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_398 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 25, 25, 64)   192         conv2d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 25, 25, 64)   0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_396 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_399 (Conv2D)             (None, 25, 25, 96)   55296       activation_398[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 25, 25, 48)   144         conv2d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 25, 25, 96)   288         conv2d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 25, 25, 48)   0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 25, 25, 96)   0           batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_38 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_395 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_397 (Conv2D)             (None, 25, 25, 64)   76800       activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_400 (Conv2D)             (None, 25, 25, 96)   82944       activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_401 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_38[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 25, 25, 64)   192         conv2d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 25, 25, 64)   192         conv2d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 25, 25, 96)   288         conv2d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 25, 25, 64)   192         conv2d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 25, 25, 64)   0           batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 25, 25, 64)   0           batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 25, 25, 96)   0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 25, 25, 64)   0           batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_395[0][0]             \n",
            "                                                                 activation_397[0][0]             \n",
            "                                                                 activation_400[0][0]             \n",
            "                                                                 activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_403 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_403 (BatchN (None, 25, 25, 64)   192         conv2d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_403 (Activation)     (None, 25, 25, 64)   0           batch_normalization_403[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_404 (Conv2D)             (None, 25, 25, 96)   55296       activation_403[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_404 (BatchN (None, 25, 25, 96)   288         conv2d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_404 (Activation)     (None, 25, 25, 96)   0           batch_normalization_404[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_402 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_405 (Conv2D)             (None, 12, 12, 96)   82944       activation_404[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_402 (BatchN (None, 12, 12, 384)  1152        conv2d_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_405 (BatchN (None, 12, 12, 96)   288         conv2d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_402 (Activation)     (None, 12, 12, 384)  0           batch_normalization_402[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_405 (Activation)     (None, 12, 12, 96)   0           batch_normalization_405[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_402[0][0]             \n",
            "                                                                 activation_405[0][0]             \n",
            "                                                                 max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_410 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_410 (BatchN (None, 12, 12, 128)  384         conv2d_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_410 (Activation)     (None, 12, 12, 128)  0           batch_normalization_410[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_411 (Conv2D)             (None, 12, 12, 128)  114688      activation_410[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_411 (BatchN (None, 12, 12, 128)  384         conv2d_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_411 (Activation)     (None, 12, 12, 128)  0           batch_normalization_411[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_407 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_412 (Conv2D)             (None, 12, 12, 128)  114688      activation_411[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_407 (BatchN (None, 12, 12, 128)  384         conv2d_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_412 (BatchN (None, 12, 12, 128)  384         conv2d_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_407 (Activation)     (None, 12, 12, 128)  0           batch_normalization_407[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_412 (Activation)     (None, 12, 12, 128)  0           batch_normalization_412[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_408 (Conv2D)             (None, 12, 12, 128)  114688      activation_407[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_413 (Conv2D)             (None, 12, 12, 128)  114688      activation_412[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_408 (BatchN (None, 12, 12, 128)  384         conv2d_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_413 (BatchN (None, 12, 12, 128)  384         conv2d_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_408 (Activation)     (None, 12, 12, 128)  0           batch_normalization_408[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_413 (Activation)     (None, 12, 12, 128)  0           batch_normalization_413[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_39 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_406 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_409 (Conv2D)             (None, 12, 12, 192)  172032      activation_408[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_414 (Conv2D)             (None, 12, 12, 192)  172032      activation_413[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_415 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_39[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_406 (BatchN (None, 12, 12, 192)  576         conv2d_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_409 (BatchN (None, 12, 12, 192)  576         conv2d_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_414 (BatchN (None, 12, 12, 192)  576         conv2d_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_415 (BatchN (None, 12, 12, 192)  576         conv2d_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_406 (Activation)     (None, 12, 12, 192)  0           batch_normalization_406[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_409 (Activation)     (None, 12, 12, 192)  0           batch_normalization_409[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_414 (Activation)     (None, 12, 12, 192)  0           batch_normalization_414[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_415 (Activation)     (None, 12, 12, 192)  0           batch_normalization_415[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_406[0][0]             \n",
            "                                                                 activation_409[0][0]             \n",
            "                                                                 activation_414[0][0]             \n",
            "                                                                 activation_415[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_420 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_420 (BatchN (None, 12, 12, 160)  480         conv2d_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_420 (Activation)     (None, 12, 12, 160)  0           batch_normalization_420[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_421 (Conv2D)             (None, 12, 12, 160)  179200      activation_420[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_421 (BatchN (None, 12, 12, 160)  480         conv2d_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_421 (Activation)     (None, 12, 12, 160)  0           batch_normalization_421[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_417 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_422 (Conv2D)             (None, 12, 12, 160)  179200      activation_421[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_417 (BatchN (None, 12, 12, 160)  480         conv2d_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_422 (BatchN (None, 12, 12, 160)  480         conv2d_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_417 (Activation)     (None, 12, 12, 160)  0           batch_normalization_417[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_422 (Activation)     (None, 12, 12, 160)  0           batch_normalization_422[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_418 (Conv2D)             (None, 12, 12, 160)  179200      activation_417[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_423 (Conv2D)             (None, 12, 12, 160)  179200      activation_422[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_418 (BatchN (None, 12, 12, 160)  480         conv2d_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_423 (BatchN (None, 12, 12, 160)  480         conv2d_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_418 (Activation)     (None, 12, 12, 160)  0           batch_normalization_418[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_423 (Activation)     (None, 12, 12, 160)  0           batch_normalization_423[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_40 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_416 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_419 (Conv2D)             (None, 12, 12, 192)  215040      activation_418[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_424 (Conv2D)             (None, 12, 12, 192)  215040      activation_423[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_425 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_40[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_416 (BatchN (None, 12, 12, 192)  576         conv2d_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_419 (BatchN (None, 12, 12, 192)  576         conv2d_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_424 (BatchN (None, 12, 12, 192)  576         conv2d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_425 (BatchN (None, 12, 12, 192)  576         conv2d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_416 (Activation)     (None, 12, 12, 192)  0           batch_normalization_416[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_419 (Activation)     (None, 12, 12, 192)  0           batch_normalization_419[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_424 (Activation)     (None, 12, 12, 192)  0           batch_normalization_424[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_425 (Activation)     (None, 12, 12, 192)  0           batch_normalization_425[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_416[0][0]             \n",
            "                                                                 activation_419[0][0]             \n",
            "                                                                 activation_424[0][0]             \n",
            "                                                                 activation_425[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_430 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_430 (BatchN (None, 12, 12, 160)  480         conv2d_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_430 (Activation)     (None, 12, 12, 160)  0           batch_normalization_430[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_431 (Conv2D)             (None, 12, 12, 160)  179200      activation_430[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_431 (BatchN (None, 12, 12, 160)  480         conv2d_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_431 (Activation)     (None, 12, 12, 160)  0           batch_normalization_431[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_427 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_432 (Conv2D)             (None, 12, 12, 160)  179200      activation_431[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_427 (BatchN (None, 12, 12, 160)  480         conv2d_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_432 (BatchN (None, 12, 12, 160)  480         conv2d_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_427 (Activation)     (None, 12, 12, 160)  0           batch_normalization_427[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_432 (Activation)     (None, 12, 12, 160)  0           batch_normalization_432[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_428 (Conv2D)             (None, 12, 12, 160)  179200      activation_427[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_433 (Conv2D)             (None, 12, 12, 160)  179200      activation_432[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_428 (BatchN (None, 12, 12, 160)  480         conv2d_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_433 (BatchN (None, 12, 12, 160)  480         conv2d_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_428 (Activation)     (None, 12, 12, 160)  0           batch_normalization_428[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_433 (Activation)     (None, 12, 12, 160)  0           batch_normalization_433[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_41 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_426 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_429 (Conv2D)             (None, 12, 12, 192)  215040      activation_428[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_434 (Conv2D)             (None, 12, 12, 192)  215040      activation_433[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_435 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_41[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_426 (BatchN (None, 12, 12, 192)  576         conv2d_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_429 (BatchN (None, 12, 12, 192)  576         conv2d_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_434 (BatchN (None, 12, 12, 192)  576         conv2d_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_435 (BatchN (None, 12, 12, 192)  576         conv2d_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_426 (Activation)     (None, 12, 12, 192)  0           batch_normalization_426[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_429 (Activation)     (None, 12, 12, 192)  0           batch_normalization_429[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_434 (Activation)     (None, 12, 12, 192)  0           batch_normalization_434[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_435 (Activation)     (None, 12, 12, 192)  0           batch_normalization_435[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_426[0][0]             \n",
            "                                                                 activation_429[0][0]             \n",
            "                                                                 activation_434[0][0]             \n",
            "                                                                 activation_435[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_440 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_440 (BatchN (None, 12, 12, 192)  576         conv2d_440[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_440 (Activation)     (None, 12, 12, 192)  0           batch_normalization_440[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_441 (Conv2D)             (None, 12, 12, 192)  258048      activation_440[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_441 (BatchN (None, 12, 12, 192)  576         conv2d_441[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_441 (Activation)     (None, 12, 12, 192)  0           batch_normalization_441[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_437 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_442 (Conv2D)             (None, 12, 12, 192)  258048      activation_441[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_437 (BatchN (None, 12, 12, 192)  576         conv2d_437[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_442 (BatchN (None, 12, 12, 192)  576         conv2d_442[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_437 (Activation)     (None, 12, 12, 192)  0           batch_normalization_437[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_442 (Activation)     (None, 12, 12, 192)  0           batch_normalization_442[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_438 (Conv2D)             (None, 12, 12, 192)  258048      activation_437[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_443 (Conv2D)             (None, 12, 12, 192)  258048      activation_442[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_438 (BatchN (None, 12, 12, 192)  576         conv2d_438[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_443 (BatchN (None, 12, 12, 192)  576         conv2d_443[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_438 (Activation)     (None, 12, 12, 192)  0           batch_normalization_438[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_443 (Activation)     (None, 12, 12, 192)  0           batch_normalization_443[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_42 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_436 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_439 (Conv2D)             (None, 12, 12, 192)  258048      activation_438[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_444 (Conv2D)             (None, 12, 12, 192)  258048      activation_443[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_445 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_42[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_436 (BatchN (None, 12, 12, 192)  576         conv2d_436[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_439 (BatchN (None, 12, 12, 192)  576         conv2d_439[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_444 (BatchN (None, 12, 12, 192)  576         conv2d_444[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_445 (BatchN (None, 12, 12, 192)  576         conv2d_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_436 (Activation)     (None, 12, 12, 192)  0           batch_normalization_436[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_439 (Activation)     (None, 12, 12, 192)  0           batch_normalization_439[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_444 (Activation)     (None, 12, 12, 192)  0           batch_normalization_444[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_445 (Activation)     (None, 12, 12, 192)  0           batch_normalization_445[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_436[0][0]             \n",
            "                                                                 activation_439[0][0]             \n",
            "                                                                 activation_444[0][0]             \n",
            "                                                                 activation_445[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_448 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_448 (BatchN (None, 12, 12, 192)  576         conv2d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_448 (Activation)     (None, 12, 12, 192)  0           batch_normalization_448[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_449 (Conv2D)             (None, 12, 12, 192)  258048      activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_449 (BatchN (None, 12, 12, 192)  576         conv2d_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_449 (Activation)     (None, 12, 12, 192)  0           batch_normalization_449[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_446 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_450 (Conv2D)             (None, 12, 12, 192)  258048      activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_446 (BatchN (None, 12, 12, 192)  576         conv2d_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_450 (BatchN (None, 12, 12, 192)  576         conv2d_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_446 (Activation)     (None, 12, 12, 192)  0           batch_normalization_446[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 12, 12, 192)  0           batch_normalization_450[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_447 (Conv2D)             (None, 5, 5, 320)    552960      activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_451 (Conv2D)             (None, 5, 5, 192)    331776      activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_447 (BatchN (None, 5, 5, 320)    960         conv2d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_451 (BatchN (None, 5, 5, 192)    576         conv2d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_447 (Activation)     (None, 5, 5, 320)    0           batch_normalization_447[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 5, 5, 192)    0           batch_normalization_451[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_447[0][0]             \n",
            "                                                                 activation_451[0][0]             \n",
            "                                                                 max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_456 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_456 (BatchN (None, 5, 5, 448)    1344        conv2d_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_456 (Activation)     (None, 5, 5, 448)    0           batch_normalization_456[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_453 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_457 (Conv2D)             (None, 5, 5, 384)    1548288     activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_453 (BatchN (None, 5, 5, 384)    1152        conv2d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_457 (BatchN (None, 5, 5, 384)    1152        conv2d_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 5, 5, 384)    0           batch_normalization_453[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_457 (Activation)     (None, 5, 5, 384)    0           batch_normalization_457[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_454 (Conv2D)             (None, 5, 5, 384)    442368      activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_455 (Conv2D)             (None, 5, 5, 384)    442368      activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_458 (Conv2D)             (None, 5, 5, 384)    442368      activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_459 (Conv2D)             (None, 5, 5, 384)    442368      activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_43 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_452 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_454 (BatchN (None, 5, 5, 384)    1152        conv2d_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_455 (BatchN (None, 5, 5, 384)    1152        conv2d_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_458 (BatchN (None, 5, 5, 384)    1152        conv2d_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_459 (BatchN (None, 5, 5, 384)    1152        conv2d_459[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_460 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_43[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_452 (BatchN (None, 5, 5, 320)    960         conv2d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 5, 5, 384)    0           batch_normalization_454[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 5, 5, 384)    0           batch_normalization_455[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_458 (Activation)     (None, 5, 5, 384)    0           batch_normalization_458[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_459 (Activation)     (None, 5, 5, 384)    0           batch_normalization_459[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_460 (BatchN (None, 5, 5, 192)    576         conv2d_460[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 5, 5, 320)    0           batch_normalization_452[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_454[0][0]             \n",
            "                                                                 activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 5, 5, 768)    0           activation_458[0][0]             \n",
            "                                                                 activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_460 (Activation)     (None, 5, 5, 192)    0           batch_normalization_460[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_452[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_8[0][0]              \n",
            "                                                                 activation_460[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_465 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_465 (BatchN (None, 5, 5, 448)    1344        conv2d_465[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_465 (Activation)     (None, 5, 5, 448)    0           batch_normalization_465[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_462 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_466 (Conv2D)             (None, 5, 5, 384)    1548288     activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_462 (BatchN (None, 5, 5, 384)    1152        conv2d_462[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_466 (BatchN (None, 5, 5, 384)    1152        conv2d_466[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_462 (Activation)     (None, 5, 5, 384)    0           batch_normalization_462[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_466 (Activation)     (None, 5, 5, 384)    0           batch_normalization_466[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_463 (Conv2D)             (None, 5, 5, 384)    442368      activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_464 (Conv2D)             (None, 5, 5, 384)    442368      activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_467 (Conv2D)             (None, 5, 5, 384)    442368      activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_468 (Conv2D)             (None, 5, 5, 384)    442368      activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_44 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_461 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_463 (BatchN (None, 5, 5, 384)    1152        conv2d_463[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_464 (BatchN (None, 5, 5, 384)    1152        conv2d_464[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_467 (BatchN (None, 5, 5, 384)    1152        conv2d_467[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_468 (BatchN (None, 5, 5, 384)    1152        conv2d_468[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_469 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_44[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_461 (BatchN (None, 5, 5, 320)    960         conv2d_461[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_463 (Activation)     (None, 5, 5, 384)    0           batch_normalization_463[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_464 (Activation)     (None, 5, 5, 384)    0           batch_normalization_464[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_467 (Activation)     (None, 5, 5, 384)    0           batch_normalization_467[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_468 (Activation)     (None, 5, 5, 384)    0           batch_normalization_468[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_469 (BatchN (None, 5, 5, 192)    576         conv2d_469[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_461 (Activation)     (None, 5, 5, 320)    0           batch_normalization_461[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_463[0][0]             \n",
            "                                                                 activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 5, 5, 768)    0           activation_467[0][0]             \n",
            "                                                                 activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_469 (Activation)     (None, 5, 5, 192)    0           batch_normalization_469[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_461[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_9[0][0]              \n",
            "                                                                 activation_469[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          1049088     global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 2)            1026        dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,852,898\n",
            "Trainable params: 22,818,466\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJnfPqP3DwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ef0eb7-fe78-40ea-cf3c-2fd8082c2b96"
      },
      "source": [
        "# training process\n",
        "file_num = 9\n",
        "epoch_num = 30\n",
        "for epoch in range(epoch_num):\n",
        "  print(\"\\n\\nstart epoch %d\" % epoch)\n",
        "  for i in range(file_num):\n",
        "    # load data\n",
        "    myData = DR_resized(True, i, 32, 0.1, (224, 224))\n",
        "    print(\"data batch %d loaded\" % i)\n",
        "\n",
        "    myModel.train(myData.train_images, myData.train_labels, epochs=1)\n",
        "    myData.clear()\n",
        "    gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "start epoch 0\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 46s 2s/step - loss: 1.0543 - accuracy: 0.5568 - val_loss: 11.6421 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.7533 - accuracy: 0.7207 - val_loss: 51.1790 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.7126 - accuracy: 0.7117 - val_loss: 1.0305 - val_accuracy: 0.7692\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.6007 - accuracy: 0.7297 - val_loss: 7.4220 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.4434 - accuracy: 0.8288 - val_loss: 6.1627 - val_accuracy: 0.4615\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.4399 - accuracy: 0.8559 - val_loss: 4.3490 - val_accuracy: 0.4615\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.4473 - accuracy: 0.8018 - val_loss: 1.4181 - val_accuracy: 0.6154\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.5094 - accuracy: 0.8018 - val_loss: 10.7611 - val_accuracy: 0.5385\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.4391 - accuracy: 0.7928 - val_loss: 11.2849 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 1\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 252ms/step - loss: 0.3794 - accuracy: 0.8288 - val_loss: 4.9232 - val_accuracy: 0.4615\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.3137 - accuracy: 0.8559 - val_loss: 5.3109 - val_accuracy: 0.6923\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.3864 - accuracy: 0.8288 - val_loss: 4.3302 - val_accuracy: 0.6923\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.4675 - accuracy: 0.8288 - val_loss: 1.7958 - val_accuracy: 0.5385\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.4127 - accuracy: 0.8288 - val_loss: 2.5825 - val_accuracy: 0.4615\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.3645 - accuracy: 0.8198 - val_loss: 5.0067 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.3785 - accuracy: 0.8378 - val_loss: 0.7572 - val_accuracy: 0.6154\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.3283 - accuracy: 0.8649 - val_loss: 0.2473 - val_accuracy: 0.9231\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.2513 - accuracy: 0.8829 - val_loss: 0.8608 - val_accuracy: 0.6923\n",
            "\n",
            "\n",
            "start epoch 2\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.2458 - accuracy: 0.8829 - val_loss: 1.2917 - val_accuracy: 0.4615\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.2343 - accuracy: 0.9009 - val_loss: 0.5929 - val_accuracy: 0.7692\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.1753 - accuracy: 0.9459 - val_loss: 0.4429 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.2673 - accuracy: 0.8919 - val_loss: 0.2284 - val_accuracy: 0.9231\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.4383 - accuracy: 0.8559 - val_loss: 0.5563 - val_accuracy: 0.7692\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.4428 - accuracy: 0.8198 - val_loss: 1.4335 - val_accuracy: 0.6154\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.2746 - accuracy: 0.8829 - val_loss: 0.4919 - val_accuracy: 0.6154\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.2666 - accuracy: 0.8919 - val_loss: 3.1552 - val_accuracy: 0.6154\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.2618 - accuracy: 0.8919 - val_loss: 0.5023 - val_accuracy: 0.7692\n",
            "\n",
            "\n",
            "start epoch 3\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.2684 - accuracy: 0.8919 - val_loss: 0.3426 - val_accuracy: 0.7692\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.2070 - accuracy: 0.9369 - val_loss: 0.9245 - val_accuracy: 0.6154\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.1949 - accuracy: 0.9099 - val_loss: 1.0798 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.1701 - accuracy: 0.9459 - val_loss: 2.2777 - val_accuracy: 0.6923\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1822 - accuracy: 0.9279 - val_loss: 2.8796 - val_accuracy: 0.6923\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1613 - accuracy: 0.9279 - val_loss: 0.5520 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1914 - accuracy: 0.9369 - val_loss: 0.9511 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.1853 - accuracy: 0.9279 - val_loss: 1.1289 - val_accuracy: 0.6923\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 251ms/step - loss: 0.2047 - accuracy: 0.9189 - val_loss: 0.3815 - val_accuracy: 0.8462\n",
            "\n",
            "\n",
            "start epoch 4\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1496 - accuracy: 0.9279 - val_loss: 0.5121 - val_accuracy: 0.6923\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.2588 - accuracy: 0.9459 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.1160 - accuracy: 0.9640 - val_loss: 0.1353 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 251ms/step - loss: 0.1470 - accuracy: 0.9459 - val_loss: 0.2198 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1666 - accuracy: 0.9279 - val_loss: 0.0977 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.1080 - accuracy: 0.9730 - val_loss: 0.1604 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.1011 - accuracy: 0.9640 - val_loss: 1.0086 - val_accuracy: 0.7692\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0883 - accuracy: 0.9640 - val_loss: 1.7982 - val_accuracy: 0.6154\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.2439 - accuracy: 0.8739 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 5\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.1742 - accuracy: 0.9099 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.2057 - accuracy: 0.9189 - val_loss: 0.6501 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.2030 - accuracy: 0.9279 - val_loss: 0.5315 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.1635 - accuracy: 0.9279 - val_loss: 0.3602 - val_accuracy: 0.7692\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.1411 - accuracy: 0.9640 - val_loss: 0.1917 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.1344 - accuracy: 0.9550 - val_loss: 0.4475 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0909 - accuracy: 0.9730 - val_loss: 0.0917 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.1631 - accuracy: 0.9550 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.1937 - accuracy: 0.9369 - val_loss: 1.3596 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 6\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.1298 - accuracy: 0.9369 - val_loss: 0.2046 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.1562 - accuracy: 0.9459 - val_loss: 0.4484 - val_accuracy: 0.7692\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.1719 - accuracy: 0.9279 - val_loss: 0.6800 - val_accuracy: 0.6923\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0781 - accuracy: 0.9730 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0914 - accuracy: 0.9730 - val_loss: 0.1525 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1089 - accuracy: 0.9369 - val_loss: 0.5342 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.1665 - accuracy: 0.9189 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1138 - accuracy: 0.9640 - val_loss: 0.2183 - val_accuracy: 0.9231\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.1112 - accuracy: 0.9369 - val_loss: 6.3695 - val_accuracy: 0.6923\n",
            "\n",
            "\n",
            "start epoch 7\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.1478 - accuracy: 0.9459 - val_loss: 0.4151 - val_accuracy: 0.8462\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.1905 - accuracy: 0.9369 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.2157 - accuracy: 0.9009 - val_loss: 0.3745 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0951 - accuracy: 0.9730 - val_loss: 3.2974 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1308 - accuracy: 0.9640 - val_loss: 8.1659 - val_accuracy: 0.4615\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.1764 - accuracy: 0.9369 - val_loss: 10.0334 - val_accuracy: 0.6923\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0903 - accuracy: 0.9640 - val_loss: 5.2528 - val_accuracy: 0.8462\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.1799 - accuracy: 0.9279 - val_loss: 1.3778 - val_accuracy: 0.8462\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.1985 - accuracy: 0.9279 - val_loss: 0.5262 - val_accuracy: 0.8462\n",
            "\n",
            "\n",
            "start epoch 8\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 238ms/step - loss: 0.1283 - accuracy: 0.9640 - val_loss: 2.5015 - val_accuracy: 0.7692\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.1172 - accuracy: 0.9550 - val_loss: 1.1822 - val_accuracy: 0.8462\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.1356 - accuracy: 0.9369 - val_loss: 0.8003 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 233ms/step - loss: 0.0721 - accuracy: 0.9910 - val_loss: 1.6979 - val_accuracy: 0.7692\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.0967 - accuracy: 0.9730 - val_loss: 0.2634 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0850 - accuracy: 0.9910 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0790 - accuracy: 0.9730 - val_loss: 0.1672 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0756 - accuracy: 0.9640 - val_loss: 1.3856 - val_accuracy: 0.7692\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0905 - accuracy: 0.9640 - val_loss: 2.2834 - val_accuracy: 0.6923\n",
            "\n",
            "\n",
            "start epoch 9\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0428 - accuracy: 0.9910 - val_loss: 0.4864 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0554 - accuracy: 0.9820 - val_loss: 1.0353 - val_accuracy: 0.8462\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.1331 - accuracy: 0.9459 - val_loss: 0.1259 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0713 - accuracy: 0.9820 - val_loss: 1.0307 - val_accuracy: 0.9231\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.1515 - accuracy: 0.9279 - val_loss: 0.1865 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0885 - accuracy: 0.9550 - val_loss: 1.7829 - val_accuracy: 0.6923\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.1456 - accuracy: 0.9459 - val_loss: 4.3754 - val_accuracy: 0.6154\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.0748 - accuracy: 0.9820 - val_loss: 12.9359 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0890 - accuracy: 0.9459 - val_loss: 8.6217 - val_accuracy: 0.3077\n",
            "\n",
            "\n",
            "start epoch 10\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.1123 - accuracy: 0.9640 - val_loss: 3.4631 - val_accuracy: 0.5385\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.1004 - accuracy: 0.9550 - val_loss: 1.4929 - val_accuracy: 0.5385\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0988 - accuracy: 0.9640 - val_loss: 0.5836 - val_accuracy: 0.7692\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.2765 - accuracy: 0.9459 - val_loss: 0.9002 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0544 - accuracy: 0.9820 - val_loss: 0.2571 - val_accuracy: 0.8462\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.1368 - accuracy: 0.9459 - val_loss: 0.2984 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.1209 - accuracy: 0.9640 - val_loss: 0.3611 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0995 - accuracy: 0.9820 - val_loss: 0.9545 - val_accuracy: 0.6923\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.1420 - accuracy: 0.9369 - val_loss: 0.2027 - val_accuracy: 0.9231\n",
            "\n",
            "\n",
            "start epoch 11\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0943 - accuracy: 0.9730 - val_loss: 0.9697 - val_accuracy: 0.6923\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.1960 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0922 - accuracy: 0.9550 - val_loss: 0.3648 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.0724 - accuracy: 0.9820 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0846 - accuracy: 0.9730 - val_loss: 3.5082e-04 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.3259 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0253 - accuracy: 0.9910 - val_loss: 1.0272 - val_accuracy: 0.7692\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 1.9915 - val_accuracy: 0.6923\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.0343 - accuracy: 0.9820 - val_loss: 0.6612 - val_accuracy: 0.8462\n",
            "\n",
            "\n",
            "start epoch 12\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0711 - accuracy: 0.9730 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0185 - accuracy: 0.9910 - val_loss: 0.1689 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0302 - accuracy: 0.9820 - val_loss: 0.3216 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 233ms/step - loss: 0.0788 - accuracy: 0.9730 - val_loss: 0.3477 - val_accuracy: 0.9231\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.0437 - accuracy: 0.9730 - val_loss: 0.5767 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0418 - accuracy: 0.9910 - val_loss: 0.3030 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0368 - accuracy: 0.9820 - val_loss: 0.1996 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0276 - accuracy: 0.9910 - val_loss: 1.2763 - val_accuracy: 0.7692\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.0816 - accuracy: 0.9820 - val_loss: 0.4198 - val_accuracy: 0.8462\n",
            "\n",
            "\n",
            "start epoch 13\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 228ms/step - loss: 0.0225 - accuracy: 0.9910 - val_loss: 0.0839 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.0401 - accuracy: 0.9910 - val_loss: 1.5932 - val_accuracy: 0.7692\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.1548 - accuracy: 0.9550 - val_loss: 0.0778 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 227ms/step - loss: 0.1441 - accuracy: 0.9369 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.2606 - accuracy: 0.9099 - val_loss: 1.0733 - val_accuracy: 0.6923\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.2091 - accuracy: 0.9279 - val_loss: 3.3466 - val_accuracy: 0.5385\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 230ms/step - loss: 0.1431 - accuracy: 0.9459 - val_loss: 11.9469 - val_accuracy: 0.5385\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.1774 - accuracy: 0.9189 - val_loss: 3.2414 - val_accuracy: 0.6923\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.1876 - accuracy: 0.9279 - val_loss: 1.1701 - val_accuracy: 0.9231\n",
            "\n",
            "\n",
            "start epoch 14\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.1159 - accuracy: 0.9640 - val_loss: 0.1980 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 233ms/step - loss: 0.0991 - accuracy: 0.9459 - val_loss: 0.1114 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0753 - accuracy: 0.9730 - val_loss: 0.6232 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.1715 - accuracy: 0.9279 - val_loss: 0.5522 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0992 - accuracy: 0.9730 - val_loss: 1.1542 - val_accuracy: 0.8462\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.1822 - accuracy: 0.9369 - val_loss: 1.2140 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0434 - accuracy: 0.9820 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0299 - accuracy: 0.9910 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.0513 - accuracy: 0.9910 - val_loss: 0.1370 - val_accuracy: 0.9231\n",
            "\n",
            "\n",
            "start epoch 15\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 237ms/step - loss: 0.0193 - accuracy: 0.9910 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 237ms/step - loss: 0.1006 - accuracy: 0.9550 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.0810 - accuracy: 0.9730 - val_loss: 2.6502e-04 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0292 - accuracy: 0.9820 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0293 - accuracy: 0.9910 - val_loss: 0.4125 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0642 - accuracy: 0.9910 - val_loss: 1.5505e-04 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.0214 - accuracy: 0.9910 - val_loss: 4.8839e-04 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0449 - accuracy: 0.9730 - val_loss: 1.4479e-04 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 16\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0813 - accuracy: 0.9459 - val_loss: 0.5030 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 2.5648 - val_accuracy: 0.6923\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0331 - accuracy: 0.9910 - val_loss: 4.2376 - val_accuracy: 0.5385\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0411 - accuracy: 0.9820 - val_loss: 1.3929 - val_accuracy: 0.5385\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0464 - accuracy: 0.9820 - val_loss: 1.8137 - val_accuracy: 0.5385\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0681 - accuracy: 0.9640 - val_loss: 0.0804 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0370 - accuracy: 0.9820 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0230 - accuracy: 0.9910 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 17\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0561 - accuracy: 0.9730 - val_loss: 5.2860e-04 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1199 - accuracy: 0.9369 - val_loss: 0.3454 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0400 - accuracy: 0.9910 - val_loss: 0.7321 - val_accuracy: 0.6923\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.1130 - accuracy: 0.9730 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0601 - accuracy: 0.9730 - val_loss: 0.1972 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 251ms/step - loss: 0.0538 - accuracy: 0.9820 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 254ms/step - loss: 0.0981 - accuracy: 0.9550 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.1526 - accuracy: 0.9550 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 18\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0514 - accuracy: 0.9910 - val_loss: 0.4522 - val_accuracy: 0.8462\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0659 - accuracy: 0.9730 - val_loss: 0.3819 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0764 - accuracy: 0.9730 - val_loss: 0.3347 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0418 - accuracy: 0.9820 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0538 - accuracy: 0.9730 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 251ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2277 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0250 - accuracy: 0.9910 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0420 - accuracy: 0.9910 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0307 - accuracy: 0.9820 - val_loss: 0.1289 - val_accuracy: 0.9231\n",
            "\n",
            "\n",
            "start epoch 19\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0582 - accuracy: 0.9730 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0225 - accuracy: 0.9910 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0246 - accuracy: 0.9820 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0356 - accuracy: 0.9820 - val_loss: 0.2160 - val_accuracy: 0.9231\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0146 - accuracy: 0.9910 - val_loss: 2.3589e-04 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1093 - accuracy: 0.9730 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0150 - accuracy: 0.9910 - val_loss: 0.4193 - val_accuracy: 0.8462\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0789 - accuracy: 0.9640 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 20\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0370 - accuracy: 0.9820 - val_loss: 0.1011 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0760 - accuracy: 0.9820 - val_loss: 0.1258 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0834 - accuracy: 0.9640 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.1415 - accuracy: 0.9459 - val_loss: 0.2939 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0617 - accuracy: 0.9730 - val_loss: 3.6257 - val_accuracy: 0.6923\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0649 - accuracy: 0.9910 - val_loss: 7.9585 - val_accuracy: 0.4615\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0398 - accuracy: 0.9910 - val_loss: 11.4634 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0869 - accuracy: 0.9640 - val_loss: 4.9146 - val_accuracy: 0.5385\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0248 - accuracy: 0.9910 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 21\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0385 - accuracy: 0.9820 - val_loss: 1.3313 - val_accuracy: 0.7692\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.1494 - accuracy: 0.9550 - val_loss: 0.1363 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0638 - accuracy: 0.9730 - val_loss: 1.0096 - val_accuracy: 0.7692\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.6273 - val_accuracy: 0.6923\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 254ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 2.7914 - val_accuracy: 0.6923\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0128 - accuracy: 0.9910 - val_loss: 0.7900 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0758 - accuracy: 0.9820 - val_loss: 1.4220 - val_accuracy: 0.7692\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0218 - accuracy: 0.9910 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.9859 - val_accuracy: 0.7692\n",
            "\n",
            "\n",
            "start epoch 22\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 253ms/step - loss: 0.0278 - accuracy: 0.9910 - val_loss: 2.4339 - val_accuracy: 0.7692\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.1577 - accuracy: 0.9730 - val_loss: 2.4673 - val_accuracy: 0.7692\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.9536 - val_accuracy: 0.8462\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.8462\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0143 - accuracy: 0.9910 - val_loss: 0.2988 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 257ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.7782 - val_accuracy: 0.8462\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3324 - val_accuracy: 0.7692\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0255 - accuracy: 0.9910 - val_loss: 0.0981 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 23\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 257ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0246 - accuracy: 0.9910 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0175 - accuracy: 0.9910 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 252ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.8462\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1713 - val_accuracy: 0.9231\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0406 - accuracy: 0.9910 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 24\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0524 - accuracy: 0.9820 - val_loss: 0.9781 - val_accuracy: 0.8462\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0340 - accuracy: 0.9820 - val_loss: 1.7849 - val_accuracy: 0.7692\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0596 - accuracy: 0.9820 - val_loss: 4.0866e-04 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0509 - accuracy: 0.9820 - val_loss: 0.6765 - val_accuracy: 0.8462\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0156 - accuracy: 0.9910 - val_loss: 0.3340 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0393 - accuracy: 0.9820 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 249ms/step - loss: 0.0444 - accuracy: 0.9820 - val_loss: 0.6542 - val_accuracy: 0.8462\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.0407 - accuracy: 0.9820 - val_loss: 0.0881 - val_accuracy: 0.9231\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0680 - accuracy: 0.9730 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 25\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0272 - accuracy: 0.9910 - val_loss: 2.9588e-05 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 7.8125e-06 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0259 - accuracy: 0.9820 - val_loss: 2.0692e-04 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.2398 - accuracy: 0.9910 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 0.9231\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 248ms/step - loss: 0.0524 - accuracy: 0.9730 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 26\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0721 - accuracy: 0.9730 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0517 - accuracy: 0.9730 - val_loss: 0.2048 - val_accuracy: 0.8462\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0481 - accuracy: 0.9730 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0912 - accuracy: 0.9640 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0566 - accuracy: 0.9730 - val_loss: 0.8861 - val_accuracy: 0.7692\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 0.6024 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 238ms/step - loss: 0.0444 - accuracy: 0.9910 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 238ms/step - loss: 0.0237 - accuracy: 0.9910 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "start epoch 27\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9231\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0212 - accuracy: 0.9820 - val_loss: 1.9300e-04 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 6.5127e-05 - val_accuracy: 1.0000\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0442 - accuracy: 0.9820 - val_loss: 0.0598 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0179 - accuracy: 0.9910 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 6.0806e-04 - accuracy: 1.0000 - val_loss: 1.4634e-05 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 8.5217e-05 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 9.1473e-04 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 0.9231\n",
            "\n",
            "\n",
            "start epoch 28\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0219 - accuracy: 0.9910 - val_loss: 1.5881e-05 - val_accuracy: 1.0000\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9457e-05 - val_accuracy: 1.0000\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0139 - accuracy: 0.9910 - val_loss: 0.0942 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0353 - accuracy: 0.9910 - val_loss: 1.6290 - val_accuracy: 0.9231\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0472 - accuracy: 0.9910 - val_loss: 1.5614 - val_accuracy: 0.7692\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0475 - accuracy: 0.9730 - val_loss: 0.3190 - val_accuracy: 0.8462\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.8462\n",
            "\n",
            "\n",
            "start epoch 29\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 1s 247ms/step - loss: 0.0209 - accuracy: 0.9910 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9231\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2423 - val_accuracy: 0.9231\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1232 - val_accuracy: 0.9231\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.1520 - accuracy: 0.9640 - val_loss: 0.3347 - val_accuracy: 0.9231\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0624 - accuracy: 0.9820 - val_loss: 5.9959e-04 - val_accuracy: 1.0000\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0749 - accuracy: 0.9820 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 1s 237ms/step - loss: 0.1040 - accuracy: 0.9730 - val_loss: 0.2144 - val_accuracy: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0svWpVPkiUsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b1e9c3-265c-43d0-a1ba-322beabe0d71"
      },
      "source": [
        "# testing process\n",
        "file_num = 2\n",
        "num_list = []\n",
        "acc_list = []\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1, (224, 224))\n",
        "  # num, acc = myModel.evaluate(myData.test_images, myData.test_labels)\n",
        "  # print(num, acc)\n",
        "  p = myModel.model.predict(myData.test_images)\n",
        "  # print(p)\n",
        "  m.update_state(p, myData.test_labels)\n",
        "  # num_list.append(num)\n",
        "  # acc_list.append(acc)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "print(m.result().numpy())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9066667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao-XE2iuqBwV"
      },
      "source": [
        "myData = DR_resized(True, 1, 32, 0.1, (224, 224))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka-Jb5ksq_BL",
        "outputId": "7790eb8e-7d8c-4e83-b85c-883da3ecf999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_labels_one_hot = myData.train_labels\n",
        "# all_labels_one_hot = np.zeros((myData.test_labels.size, 2))\n",
        "# all_labels_one_hot[np.arange(myData.test_labels.size),myData.test_labels] = 1\n",
        "\n",
        "p = myModel.model.predict(myData.train_images)\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "m.update_state(p[-10:], all_labels_one_hot[-10:])\n",
        "m.result().numpy()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x5SZXUUyzNB",
        "outputId": "a09464ae-b964-4bc2-8963-f8ce1dfb4191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "p.argmax(1)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f71I1uqy1P2",
        "outputId": "34f356ec-c457-45d1-9eba-e7253ffcaf7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_labels_one_hot[-10:]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL9za1Dkqjy"
      },
      "source": [
        "# compute overall accuracy\n",
        "num_list = np.array(num_list)\n",
        "acc_list = np.array(acc_list)\n",
        "acc_overall = (num_list * acc_list).sum() / num_list.sum()\n",
        "print(acc_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQpdk7vC25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37861ac-8284-420f-eb05-2739f9c1ab77"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(\"date and time =\", dt_string)\n",
        "myModel.save_model(os.path.join(project_root, \"Trained Models\", dt_string))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date and time = 04-04-2021 21:07:02\n",
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/CS 766 Project/Project Coding and Data Files/Trained Models/04-04-2021 21:07:02/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QDEecGwGTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9eeed3-e6de-479f-af9f-3c5650757151"
      },
      "source": [
        "# this part is for loading parameters and extracting features\n",
        "# load parameters\n",
        "parameter_version = \"04-04-2021 21:07:02\"\n",
        "if not os.path.exists(os.path.join(project_root, \"features\", parameter_version)):\n",
        "    os.mkdir(os.path.join(project_root, \"features\", parameter_version))\n",
        "\n",
        "# my_model_pretrained = OriginCNN()\n",
        "my_model_pretrained = Res50()\n",
        "my_model_pretrained.build()\n",
        "my_model_pretrained.load_model(os.path.join(project_root, \"Trained Models\", parameter_version))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_658 (Conv2D)             (None, 111, 111, 32) 864         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_658 (BatchN (None, 111, 111, 32) 96          conv2d_658[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_658 (Activation)     (None, 111, 111, 32) 0           batch_normalization_658[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_659 (Conv2D)             (None, 109, 109, 32) 9216        activation_658[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_659 (BatchN (None, 109, 109, 32) 96          conv2d_659[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_659 (Activation)     (None, 109, 109, 32) 0           batch_normalization_659[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_660 (Conv2D)             (None, 109, 109, 64) 18432       activation_659[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_660 (BatchN (None, 109, 109, 64) 192         conv2d_660[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_660 (Activation)     (None, 109, 109, 64) 0           batch_normalization_660[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling2D) (None, 54, 54, 64)   0           activation_660[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_661 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_28[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_661 (BatchN (None, 54, 54, 80)   240         conv2d_661[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_661 (Activation)     (None, 54, 54, 80)   0           batch_normalization_661[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_662 (Conv2D)             (None, 52, 52, 192)  138240      activation_661[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_662 (BatchN (None, 52, 52, 192)  576         conv2d_662[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_662 (Activation)     (None, 52, 52, 192)  0           batch_normalization_662[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling2D) (None, 25, 25, 192)  0           activation_662[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_666 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_666 (BatchN (None, 25, 25, 64)   192         conv2d_666[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_666 (Activation)     (None, 25, 25, 64)   0           batch_normalization_666[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_664 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_667 (Conv2D)             (None, 25, 25, 96)   55296       activation_666[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_664 (BatchN (None, 25, 25, 48)   144         conv2d_664[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_667 (BatchN (None, 25, 25, 96)   288         conv2d_667[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_664 (Activation)     (None, 25, 25, 48)   0           batch_normalization_664[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_667 (Activation)     (None, 25, 25, 96)   0           batch_normalization_667[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_63 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_663 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_29[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_665 (Conv2D)             (None, 25, 25, 64)   76800       activation_664[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_668 (Conv2D)             (None, 25, 25, 96)   82944       activation_667[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_669 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_63[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_663 (BatchN (None, 25, 25, 64)   192         conv2d_663[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_665 (BatchN (None, 25, 25, 64)   192         conv2d_665[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_668 (BatchN (None, 25, 25, 96)   288         conv2d_668[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_669 (BatchN (None, 25, 25, 32)   96          conv2d_669[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_663 (Activation)     (None, 25, 25, 64)   0           batch_normalization_663[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_665 (Activation)     (None, 25, 25, 64)   0           batch_normalization_665[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_668 (Activation)     (None, 25, 25, 96)   0           batch_normalization_668[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_669 (Activation)     (None, 25, 25, 32)   0           batch_normalization_669[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_663[0][0]             \n",
            "                                                                 activation_665[0][0]             \n",
            "                                                                 activation_668[0][0]             \n",
            "                                                                 activation_669[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_673 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_673 (BatchN (None, 25, 25, 64)   192         conv2d_673[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_673 (Activation)     (None, 25, 25, 64)   0           batch_normalization_673[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_671 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_674 (Conv2D)             (None, 25, 25, 96)   55296       activation_673[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_671 (BatchN (None, 25, 25, 48)   144         conv2d_671[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_674 (BatchN (None, 25, 25, 96)   288         conv2d_674[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_671 (Activation)     (None, 25, 25, 48)   0           batch_normalization_671[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_674 (Activation)     (None, 25, 25, 96)   0           batch_normalization_674[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_64 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_670 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_672 (Conv2D)             (None, 25, 25, 64)   76800       activation_671[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_675 (Conv2D)             (None, 25, 25, 96)   82944       activation_674[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_676 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_64[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_670 (BatchN (None, 25, 25, 64)   192         conv2d_670[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_672 (BatchN (None, 25, 25, 64)   192         conv2d_672[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_675 (BatchN (None, 25, 25, 96)   288         conv2d_675[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_676 (BatchN (None, 25, 25, 64)   192         conv2d_676[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_670 (Activation)     (None, 25, 25, 64)   0           batch_normalization_670[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_672 (Activation)     (None, 25, 25, 64)   0           batch_normalization_672[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_675 (Activation)     (None, 25, 25, 96)   0           batch_normalization_675[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_676 (Activation)     (None, 25, 25, 64)   0           batch_normalization_676[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_670[0][0]             \n",
            "                                                                 activation_672[0][0]             \n",
            "                                                                 activation_675[0][0]             \n",
            "                                                                 activation_676[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_680 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_680 (BatchN (None, 25, 25, 64)   192         conv2d_680[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_680 (Activation)     (None, 25, 25, 64)   0           batch_normalization_680[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_678 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_681 (Conv2D)             (None, 25, 25, 96)   55296       activation_680[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_678 (BatchN (None, 25, 25, 48)   144         conv2d_678[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_681 (BatchN (None, 25, 25, 96)   288         conv2d_681[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_678 (Activation)     (None, 25, 25, 48)   0           batch_normalization_678[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_681 (Activation)     (None, 25, 25, 96)   0           batch_normalization_681[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_65 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_677 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_679 (Conv2D)             (None, 25, 25, 64)   76800       activation_678[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_682 (Conv2D)             (None, 25, 25, 96)   82944       activation_681[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_683 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_65[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_677 (BatchN (None, 25, 25, 64)   192         conv2d_677[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_679 (BatchN (None, 25, 25, 64)   192         conv2d_679[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_682 (BatchN (None, 25, 25, 96)   288         conv2d_682[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_683 (BatchN (None, 25, 25, 64)   192         conv2d_683[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_677 (Activation)     (None, 25, 25, 64)   0           batch_normalization_677[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_679 (Activation)     (None, 25, 25, 64)   0           batch_normalization_679[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_682 (Activation)     (None, 25, 25, 96)   0           batch_normalization_682[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_683 (Activation)     (None, 25, 25, 64)   0           batch_normalization_683[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_677[0][0]             \n",
            "                                                                 activation_679[0][0]             \n",
            "                                                                 activation_682[0][0]             \n",
            "                                                                 activation_683[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_685 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_685 (BatchN (None, 25, 25, 64)   192         conv2d_685[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_685 (Activation)     (None, 25, 25, 64)   0           batch_normalization_685[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_686 (Conv2D)             (None, 25, 25, 96)   55296       activation_685[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_686 (BatchN (None, 25, 25, 96)   288         conv2d_686[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_686 (Activation)     (None, 25, 25, 96)   0           batch_normalization_686[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_684 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_687 (Conv2D)             (None, 12, 12, 96)   82944       activation_686[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_684 (BatchN (None, 12, 12, 384)  1152        conv2d_684[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_687 (BatchN (None, 12, 12, 96)   288         conv2d_687[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_684 (Activation)     (None, 12, 12, 384)  0           batch_normalization_684[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_687 (Activation)     (None, 12, 12, 96)   0           batch_normalization_687[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_684[0][0]             \n",
            "                                                                 activation_687[0][0]             \n",
            "                                                                 max_pooling2d_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_692 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_692 (BatchN (None, 12, 12, 128)  384         conv2d_692[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_692 (Activation)     (None, 12, 12, 128)  0           batch_normalization_692[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_693 (Conv2D)             (None, 12, 12, 128)  114688      activation_692[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_693 (BatchN (None, 12, 12, 128)  384         conv2d_693[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_693 (Activation)     (None, 12, 12, 128)  0           batch_normalization_693[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_689 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_694 (Conv2D)             (None, 12, 12, 128)  114688      activation_693[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_689 (BatchN (None, 12, 12, 128)  384         conv2d_689[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_694 (BatchN (None, 12, 12, 128)  384         conv2d_694[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_689 (Activation)     (None, 12, 12, 128)  0           batch_normalization_689[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_694 (Activation)     (None, 12, 12, 128)  0           batch_normalization_694[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_690 (Conv2D)             (None, 12, 12, 128)  114688      activation_689[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_695 (Conv2D)             (None, 12, 12, 128)  114688      activation_694[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_690 (BatchN (None, 12, 12, 128)  384         conv2d_690[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_695 (BatchN (None, 12, 12, 128)  384         conv2d_695[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_690 (Activation)     (None, 12, 12, 128)  0           batch_normalization_690[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_695 (Activation)     (None, 12, 12, 128)  0           batch_normalization_695[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_66 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_688 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_691 (Conv2D)             (None, 12, 12, 192)  172032      activation_690[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_696 (Conv2D)             (None, 12, 12, 192)  172032      activation_695[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_697 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_66[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_688 (BatchN (None, 12, 12, 192)  576         conv2d_688[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_691 (BatchN (None, 12, 12, 192)  576         conv2d_691[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_696 (BatchN (None, 12, 12, 192)  576         conv2d_696[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_697 (BatchN (None, 12, 12, 192)  576         conv2d_697[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_688 (Activation)     (None, 12, 12, 192)  0           batch_normalization_688[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_691 (Activation)     (None, 12, 12, 192)  0           batch_normalization_691[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_696 (Activation)     (None, 12, 12, 192)  0           batch_normalization_696[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_697 (Activation)     (None, 12, 12, 192)  0           batch_normalization_697[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_688[0][0]             \n",
            "                                                                 activation_691[0][0]             \n",
            "                                                                 activation_696[0][0]             \n",
            "                                                                 activation_697[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_702 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_702 (BatchN (None, 12, 12, 160)  480         conv2d_702[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_702 (Activation)     (None, 12, 12, 160)  0           batch_normalization_702[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_703 (Conv2D)             (None, 12, 12, 160)  179200      activation_702[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_703 (BatchN (None, 12, 12, 160)  480         conv2d_703[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_703 (Activation)     (None, 12, 12, 160)  0           batch_normalization_703[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_699 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_704 (Conv2D)             (None, 12, 12, 160)  179200      activation_703[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_699 (BatchN (None, 12, 12, 160)  480         conv2d_699[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_704 (BatchN (None, 12, 12, 160)  480         conv2d_704[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_699 (Activation)     (None, 12, 12, 160)  0           batch_normalization_699[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_704 (Activation)     (None, 12, 12, 160)  0           batch_normalization_704[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_700 (Conv2D)             (None, 12, 12, 160)  179200      activation_699[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_705 (Conv2D)             (None, 12, 12, 160)  179200      activation_704[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_700 (BatchN (None, 12, 12, 160)  480         conv2d_700[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_705 (BatchN (None, 12, 12, 160)  480         conv2d_705[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_700 (Activation)     (None, 12, 12, 160)  0           batch_normalization_700[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_705 (Activation)     (None, 12, 12, 160)  0           batch_normalization_705[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_67 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_698 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_701 (Conv2D)             (None, 12, 12, 192)  215040      activation_700[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_706 (Conv2D)             (None, 12, 12, 192)  215040      activation_705[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_707 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_67[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_698 (BatchN (None, 12, 12, 192)  576         conv2d_698[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_701 (BatchN (None, 12, 12, 192)  576         conv2d_701[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_706 (BatchN (None, 12, 12, 192)  576         conv2d_706[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_707 (BatchN (None, 12, 12, 192)  576         conv2d_707[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_698 (Activation)     (None, 12, 12, 192)  0           batch_normalization_698[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_701 (Activation)     (None, 12, 12, 192)  0           batch_normalization_701[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_706 (Activation)     (None, 12, 12, 192)  0           batch_normalization_706[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_707 (Activation)     (None, 12, 12, 192)  0           batch_normalization_707[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_698[0][0]             \n",
            "                                                                 activation_701[0][0]             \n",
            "                                                                 activation_706[0][0]             \n",
            "                                                                 activation_707[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_712 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_712 (BatchN (None, 12, 12, 160)  480         conv2d_712[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_712 (Activation)     (None, 12, 12, 160)  0           batch_normalization_712[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_713 (Conv2D)             (None, 12, 12, 160)  179200      activation_712[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_713 (BatchN (None, 12, 12, 160)  480         conv2d_713[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_713 (Activation)     (None, 12, 12, 160)  0           batch_normalization_713[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_709 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_714 (Conv2D)             (None, 12, 12, 160)  179200      activation_713[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_709 (BatchN (None, 12, 12, 160)  480         conv2d_709[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_714 (BatchN (None, 12, 12, 160)  480         conv2d_714[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_709 (Activation)     (None, 12, 12, 160)  0           batch_normalization_709[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_714 (Activation)     (None, 12, 12, 160)  0           batch_normalization_714[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_710 (Conv2D)             (None, 12, 12, 160)  179200      activation_709[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_715 (Conv2D)             (None, 12, 12, 160)  179200      activation_714[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_710 (BatchN (None, 12, 12, 160)  480         conv2d_710[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_715 (BatchN (None, 12, 12, 160)  480         conv2d_715[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_710 (Activation)     (None, 12, 12, 160)  0           batch_normalization_710[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_715 (Activation)     (None, 12, 12, 160)  0           batch_normalization_715[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_68 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_708 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_711 (Conv2D)             (None, 12, 12, 192)  215040      activation_710[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_716 (Conv2D)             (None, 12, 12, 192)  215040      activation_715[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_717 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_68[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_708 (BatchN (None, 12, 12, 192)  576         conv2d_708[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_711 (BatchN (None, 12, 12, 192)  576         conv2d_711[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_716 (BatchN (None, 12, 12, 192)  576         conv2d_716[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_717 (BatchN (None, 12, 12, 192)  576         conv2d_717[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_708 (Activation)     (None, 12, 12, 192)  0           batch_normalization_708[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_711 (Activation)     (None, 12, 12, 192)  0           batch_normalization_711[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_716 (Activation)     (None, 12, 12, 192)  0           batch_normalization_716[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_717 (Activation)     (None, 12, 12, 192)  0           batch_normalization_717[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_708[0][0]             \n",
            "                                                                 activation_711[0][0]             \n",
            "                                                                 activation_716[0][0]             \n",
            "                                                                 activation_717[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_722 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_722 (BatchN (None, 12, 12, 192)  576         conv2d_722[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_722 (Activation)     (None, 12, 12, 192)  0           batch_normalization_722[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_723 (Conv2D)             (None, 12, 12, 192)  258048      activation_722[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_723 (BatchN (None, 12, 12, 192)  576         conv2d_723[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_723 (Activation)     (None, 12, 12, 192)  0           batch_normalization_723[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_719 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_724 (Conv2D)             (None, 12, 12, 192)  258048      activation_723[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_719 (BatchN (None, 12, 12, 192)  576         conv2d_719[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_724 (BatchN (None, 12, 12, 192)  576         conv2d_724[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_719 (Activation)     (None, 12, 12, 192)  0           batch_normalization_719[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_724 (Activation)     (None, 12, 12, 192)  0           batch_normalization_724[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_720 (Conv2D)             (None, 12, 12, 192)  258048      activation_719[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_725 (Conv2D)             (None, 12, 12, 192)  258048      activation_724[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_720 (BatchN (None, 12, 12, 192)  576         conv2d_720[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_725 (BatchN (None, 12, 12, 192)  576         conv2d_725[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_720 (Activation)     (None, 12, 12, 192)  0           batch_normalization_720[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_725 (Activation)     (None, 12, 12, 192)  0           batch_normalization_725[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_69 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_718 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_721 (Conv2D)             (None, 12, 12, 192)  258048      activation_720[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_726 (Conv2D)             (None, 12, 12, 192)  258048      activation_725[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_727 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_69[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_718 (BatchN (None, 12, 12, 192)  576         conv2d_718[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_721 (BatchN (None, 12, 12, 192)  576         conv2d_721[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_726 (BatchN (None, 12, 12, 192)  576         conv2d_726[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_727 (BatchN (None, 12, 12, 192)  576         conv2d_727[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_718 (Activation)     (None, 12, 12, 192)  0           batch_normalization_718[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_721 (Activation)     (None, 12, 12, 192)  0           batch_normalization_721[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_726 (Activation)     (None, 12, 12, 192)  0           batch_normalization_726[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_727 (Activation)     (None, 12, 12, 192)  0           batch_normalization_727[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_718[0][0]             \n",
            "                                                                 activation_721[0][0]             \n",
            "                                                                 activation_726[0][0]             \n",
            "                                                                 activation_727[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_730 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_730 (BatchN (None, 12, 12, 192)  576         conv2d_730[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_730 (Activation)     (None, 12, 12, 192)  0           batch_normalization_730[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_731 (Conv2D)             (None, 12, 12, 192)  258048      activation_730[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_731 (BatchN (None, 12, 12, 192)  576         conv2d_731[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_731 (Activation)     (None, 12, 12, 192)  0           batch_normalization_731[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_728 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_732 (Conv2D)             (None, 12, 12, 192)  258048      activation_731[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_728 (BatchN (None, 12, 12, 192)  576         conv2d_728[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_732 (BatchN (None, 12, 12, 192)  576         conv2d_732[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_728 (Activation)     (None, 12, 12, 192)  0           batch_normalization_728[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_732 (Activation)     (None, 12, 12, 192)  0           batch_normalization_732[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_729 (Conv2D)             (None, 5, 5, 320)    552960      activation_728[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_733 (Conv2D)             (None, 5, 5, 192)    331776      activation_732[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_729 (BatchN (None, 5, 5, 320)    960         conv2d_729[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_733 (BatchN (None, 5, 5, 192)    576         conv2d_733[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_729 (Activation)     (None, 5, 5, 320)    0           batch_normalization_729[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_733 (Activation)     (None, 5, 5, 192)    0           batch_normalization_733[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_729[0][0]             \n",
            "                                                                 activation_733[0][0]             \n",
            "                                                                 max_pooling2d_31[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_738 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_738 (BatchN (None, 5, 5, 448)    1344        conv2d_738[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_738 (Activation)     (None, 5, 5, 448)    0           batch_normalization_738[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_735 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_739 (Conv2D)             (None, 5, 5, 384)    1548288     activation_738[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_735 (BatchN (None, 5, 5, 384)    1152        conv2d_735[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_739 (BatchN (None, 5, 5, 384)    1152        conv2d_739[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_735 (Activation)     (None, 5, 5, 384)    0           batch_normalization_735[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_739 (Activation)     (None, 5, 5, 384)    0           batch_normalization_739[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_736 (Conv2D)             (None, 5, 5, 384)    442368      activation_735[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_737 (Conv2D)             (None, 5, 5, 384)    442368      activation_735[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_740 (Conv2D)             (None, 5, 5, 384)    442368      activation_739[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_741 (Conv2D)             (None, 5, 5, 384)    442368      activation_739[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_70 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_734 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_736 (BatchN (None, 5, 5, 384)    1152        conv2d_736[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_737 (BatchN (None, 5, 5, 384)    1152        conv2d_737[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_740 (BatchN (None, 5, 5, 384)    1152        conv2d_740[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_741 (BatchN (None, 5, 5, 384)    1152        conv2d_741[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_742 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_70[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_734 (BatchN (None, 5, 5, 320)    960         conv2d_734[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_736 (Activation)     (None, 5, 5, 384)    0           batch_normalization_736[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_737 (Activation)     (None, 5, 5, 384)    0           batch_normalization_737[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_740 (Activation)     (None, 5, 5, 384)    0           batch_normalization_740[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_741 (Activation)     (None, 5, 5, 384)    0           batch_normalization_741[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_742 (BatchN (None, 5, 5, 192)    576         conv2d_742[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_734 (Activation)     (None, 5, 5, 320)    0           batch_normalization_734[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_736[0][0]             \n",
            "                                                                 activation_737[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5, 5, 768)    0           activation_740[0][0]             \n",
            "                                                                 activation_741[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_742 (Activation)     (None, 5, 5, 192)    0           batch_normalization_742[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_734[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_14[0][0]             \n",
            "                                                                 activation_742[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_747 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_747 (BatchN (None, 5, 5, 448)    1344        conv2d_747[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_747 (Activation)     (None, 5, 5, 448)    0           batch_normalization_747[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_744 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_748 (Conv2D)             (None, 5, 5, 384)    1548288     activation_747[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_744 (BatchN (None, 5, 5, 384)    1152        conv2d_744[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_748 (BatchN (None, 5, 5, 384)    1152        conv2d_748[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_744 (Activation)     (None, 5, 5, 384)    0           batch_normalization_744[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_748 (Activation)     (None, 5, 5, 384)    0           batch_normalization_748[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_745 (Conv2D)             (None, 5, 5, 384)    442368      activation_744[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_746 (Conv2D)             (None, 5, 5, 384)    442368      activation_744[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_749 (Conv2D)             (None, 5, 5, 384)    442368      activation_748[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_750 (Conv2D)             (None, 5, 5, 384)    442368      activation_748[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_71 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_743 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_745 (BatchN (None, 5, 5, 384)    1152        conv2d_745[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_746 (BatchN (None, 5, 5, 384)    1152        conv2d_746[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_749 (BatchN (None, 5, 5, 384)    1152        conv2d_749[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_750 (BatchN (None, 5, 5, 384)    1152        conv2d_750[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_751 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_71[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_743 (BatchN (None, 5, 5, 320)    960         conv2d_743[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_745 (Activation)     (None, 5, 5, 384)    0           batch_normalization_745[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_746 (Activation)     (None, 5, 5, 384)    0           batch_normalization_746[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_749 (Activation)     (None, 5, 5, 384)    0           batch_normalization_749[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_750 (Activation)     (None, 5, 5, 384)    0           batch_normalization_750[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_751 (BatchN (None, 5, 5, 192)    576         conv2d_751[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_743 (Activation)     (None, 5, 5, 320)    0           batch_normalization_743[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_745[0][0]             \n",
            "                                                                 activation_746[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 5, 5, 768)    0           activation_749[0][0]             \n",
            "                                                                 activation_750[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_751 (Activation)     (None, 5, 5, 192)    0           batch_normalization_751[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_743[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_15[0][0]             \n",
            "                                                                 activation_751[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 512)          1049088     global_average_pooling2d_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 2)            1026        dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 22,852,898\n",
            "Trainable params: 22,818,466\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dC4KT0HjTqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be15daf5-09e2-4ba4-ebfd-2a403a37ec5c"
      },
      "source": [
        "# load training data, extract features and save to npy files\n",
        "file_num = 9\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(True, i, 32, 0.1, (224, 224))\n",
        "  feature = my_model_pretrained.extract_feature(myData.train_images)\n",
        "  # feature = myModel.extract_feature(myData.train_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtrain_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtrain_feature%d.npy\" % i)\n",
        "  print(feature.shape)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "# load testing data, extract features and save to npy files\n",
        "file_num = 2\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1, (224, 224))\n",
        "  feature = my_model_pretrained.extract_feature(myData.test_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtest_feature%d.npy\" % i)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain_feature0.npy\n",
            "(124, 512)\n",
            "Xtrain_feature1.npy\n",
            "(124, 512)\n",
            "Xtrain_feature2.npy\n",
            "(124, 512)\n",
            "Xtrain_feature3.npy\n",
            "(124, 512)\n",
            "Xtrain_feature4.npy\n",
            "(124, 512)\n",
            "Xtrain_feature5.npy\n",
            "(124, 512)\n",
            "Xtrain_feature6.npy\n",
            "(124, 512)\n",
            "Xtrain_feature7.npy\n",
            "(124, 512)\n",
            "Xtrain_feature8.npy\n",
            "(124, 512)\n",
            "Xtest_feature0.npy\n",
            "Xtest_feature1.npy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}