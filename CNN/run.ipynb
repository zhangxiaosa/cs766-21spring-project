{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoAP5Dn3DwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, re, sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYucCZeH_dHH",
        "outputId": "e26ca18d-d9a3-4ed2-8e39-19275153f13d"
      },
      "source": [
        "# test if GPUs are available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiMh4isCHnS",
        "outputId": "f382ad74-6a92-452e-b91c-9af5c7765ca8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQvxwJFDmYk"
      },
      "source": [
        "# set project root, maybe you need to firstly \n",
        "# add shortcut of CS 766 Project to drive.\n",
        "project_root = './drive/MyDrive/CS 766 Project/Project Coding and Data Files'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJsefpJ-XjF"
      },
      "source": [
        "# class to initialize CNNs\n",
        "class OriginCNN(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = 'categorical_crossentropy'\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1000, 1000, 3)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs):\n",
        "    self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-3].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3suc98v-bRj"
      },
      "source": [
        "class DR_resized(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "      \n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOADsxqM3DwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d60625e-df06-4ac8-f47e-7091acdd2102"
      },
      "source": [
        "# initialize CNN\n",
        "myModel = OriginCNN()\n",
        "myModel.build()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 333, 333, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,973\n",
            "Trainable params: 35,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJnfPqP3DwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df76311-a989-43b3-87a7-d687d637cc6b"
      },
      "source": [
        "# training process\n",
        "file_num = 9\n",
        "epoch_num = 5\n",
        "for i in range(file_num):\n",
        "  # load data\n",
        "  myData = DR_resized(True, i, 32, 0.1)\n",
        "  print(\"\\n\\ndata batch %d loaded\" % i)\n",
        "\n",
        "  for j in range(epoch_num):\n",
        "    print(\"Start of epoch %d\" % j)\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    # matric\n",
        "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    # train val split\n",
        "    val_num = round(myData.train_val_split_rate * myData.train_images.shape[0]);\n",
        "    # train dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((myData.train_images[:-1 * val_num], myData.train_labels[:-1 * val_num]))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(myData.batch_size)\n",
        "    # val dataset\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((myData.train_images[-1 * val_num:], myData.train_labels[-1 * val_num:]))\n",
        "    val_dataset = val_dataset.shuffle(buffer_size=1024).batch(myData.batch_size)\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        loss_value = myModel.train_on_batch(x_batch_train, y_batch_train);\n",
        "\n",
        "        # Update training metric.\n",
        "        # train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every n batches.\n",
        "        if step % 10 == 0:\n",
        "          train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "          train_logits = myModel.predict(x_batch_train)\n",
        "          # Update val metrics\n",
        "          train_logits = np.expand_dims(train_logits.argmax(axis=1), axis=-1)\n",
        "          y_batch_train = tf.expand_dims(tf.argmax(y_batch_train, axis=1), -1)\n",
        "          train_acc_metric.update_state(train_logits, y_batch_train)\n",
        "\n",
        "          train_acc = train_acc_metric.result()\n",
        "          train_acc_metric.reset_states()\n",
        "\n",
        "          print(\n",
        "              \"   Training loss (for one batch) at step %d: %.4f\"\n",
        "              % (step, sum(loss_value) / len(loss_value))\n",
        "          )\n",
        "          print(\n",
        "              \"   Training accuracy (for one batch) at step %d: %.4f\"\n",
        "              % (step, train_acc)\n",
        "          )\n",
        "          print(\"   Seen so far: %d samples\\n\" % ((step + 1) * 32))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    # train_acc = train_acc_metric.result()\n",
        "    # print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    # train_acc_metric.reset_states()\n",
        "\n",
        "  # Run a validation loop at the end of each epoch.\n",
        "  # matric\n",
        "  val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_logits = myModel.predict(x_batch_val)\n",
        "    # Update val metrics\n",
        "    val_logits = np.expand_dims(val_logits.argmax(axis=1), axis=-1)\n",
        "    y_batch_val = tf.expand_dims(tf.argmax(y_batch_val, axis=1), -1)\n",
        "    val_acc_metric.update_state(val_logits, y_batch_val)\n",
        "\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "  print(\"data batch %d trained, time taken: %.2fs\" % (i, time.time() - start_time))\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "data batch 0 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9734\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.8815\n",
            "   Training accuracy (for one batch) at step 0: 0.8750\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.8670\n",
            "   Training accuracy (for one batch) at step 0: 0.9688\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9125\n",
            "   Training accuracy (for one batch) at step 0: 0.7500\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9241\n",
            "   Training accuracy (for one batch) at step 0: 0.6250\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.4839\n",
            "data batch 0 trained, time taken: 5.05s\n",
            "\n",
            "\n",
            "data batch 1 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.8887\n",
            "   Training accuracy (for one batch) at step 0: 0.5625\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9083\n",
            "   Training accuracy (for one batch) at step 0: 1.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.8902\n",
            "   Training accuracy (for one batch) at step 0: 0.5000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9001\n",
            "   Training accuracy (for one batch) at step 0: 0.6562\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 1.0031\n",
            "   Training accuracy (for one batch) at step 0: 0.6250\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.6129\n",
            "data batch 1 trained, time taken: 3.42s\n",
            "\n",
            "\n",
            "data batch 2 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9541\n",
            "   Training accuracy (for one batch) at step 0: 0.5625\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.8548\n",
            "   Training accuracy (for one batch) at step 0: 0.2812\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.9108\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.8977\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9398\n",
            "   Training accuracy (for one batch) at step 0: 0.0312\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.0323\n",
            "data batch 2 trained, time taken: 3.57s\n",
            "\n",
            "\n",
            "data batch 3 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9357\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9587\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.8879\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9445\n",
            "   Training accuracy (for one batch) at step 0: 0.0938\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9201\n",
            "   Training accuracy (for one batch) at step 0: 0.2500\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.7097\n",
            "data batch 3 trained, time taken: 3.45s\n",
            "\n",
            "\n",
            "data batch 4 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9332\n",
            "   Training accuracy (for one batch) at step 0: 0.6562\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9314\n",
            "   Training accuracy (for one batch) at step 0: 0.0625\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.9237\n",
            "   Training accuracy (for one batch) at step 0: 0.1875\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9208\n",
            "   Training accuracy (for one batch) at step 0: 0.1562\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9603\n",
            "   Training accuracy (for one batch) at step 0: 0.3125\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.5484\n",
            "data batch 4 trained, time taken: 3.54s\n",
            "\n",
            "\n",
            "data batch 5 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.8500\n",
            "   Training accuracy (for one batch) at step 0: 0.6250\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9121\n",
            "   Training accuracy (for one batch) at step 0: 0.5312\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.9387\n",
            "   Training accuracy (for one batch) at step 0: 0.1250\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.8828\n",
            "   Training accuracy (for one batch) at step 0: 0.0938\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9928\n",
            "   Training accuracy (for one batch) at step 0: 0.2500\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.0323\n",
            "data batch 5 trained, time taken: 3.53s\n",
            "\n",
            "\n",
            "data batch 6 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9242\n",
            "   Training accuracy (for one batch) at step 0: 0.0000\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9358\n",
            "   Training accuracy (for one batch) at step 0: 0.0312\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.9147\n",
            "   Training accuracy (for one batch) at step 0: 0.0938\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9435\n",
            "   Training accuracy (for one batch) at step 0: 0.0625\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9483\n",
            "   Training accuracy (for one batch) at step 0: 0.2500\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.1290\n",
            "data batch 6 trained, time taken: 3.64s\n",
            "\n",
            "\n",
            "data batch 7 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9133\n",
            "   Training accuracy (for one batch) at step 0: 0.0938\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9035\n",
            "   Training accuracy (for one batch) at step 0: 0.2188\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.8892\n",
            "   Training accuracy (for one batch) at step 0: 0.3438\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9195\n",
            "   Training accuracy (for one batch) at step 0: 0.3125\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9422\n",
            "   Training accuracy (for one batch) at step 0: 0.2812\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.2258\n",
            "data batch 7 trained, time taken: 3.56s\n",
            "\n",
            "\n",
            "data batch 8 loaded\n",
            "Start of epoch 0\n",
            "   Training loss (for one batch) at step 0: 0.9052\n",
            "   Training accuracy (for one batch) at step 0: 0.2812\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 1\n",
            "   Training loss (for one batch) at step 0: 0.9718\n",
            "   Training accuracy (for one batch) at step 0: 0.2188\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 2\n",
            "   Training loss (for one batch) at step 0: 0.9275\n",
            "   Training accuracy (for one batch) at step 0: 0.0938\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 3\n",
            "   Training loss (for one batch) at step 0: 0.9404\n",
            "   Training accuracy (for one batch) at step 0: 0.0312\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Start of epoch 4\n",
            "   Training loss (for one batch) at step 0: 0.9051\n",
            "   Training accuracy (for one batch) at step 0: 0.0312\n",
            "   Seen so far: 32 samples\n",
            "\n",
            "Validation acc: 0.0323\n",
            "data batch 8 trained, time taken: 3.64s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0svWpVPkiUsK",
        "outputId": "61d4190e-d020-456d-865a-77531c93bafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# testing process\n",
        "file_num = 2\n",
        "num_list = []\n",
        "acc_list = []\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  num, acc = myModel.evaluate(myData.test_images, myData.test_labels)\n",
        "  print(num, acc)\n",
        "  num_list.append(num)\n",
        "  acc_list.append(acc)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 - 3s - loss: 1.5850 - accuracy: 0.2387\n",
            "310 0.23870967328548431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL9za1Dkqjy"
      },
      "source": [
        "# compute overall accuracy\n",
        "num_list = np.array(num_list)\n",
        "acc_list = np.array(acc_list)\n",
        "acc_overall = (num_list * acc_list).sum() / num_list.sum()\n",
        "print(acc_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQpdk7vC25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11329b10-2bf3-4f70-e080-94b9837fa877"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(\"date and time =\", dt_string)\n",
        "myModel.save_model(os.path.join(project_root, \"Trained Models\", dt_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date and time = 23-03-2021 19:18:56\n",
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/CS 766 Project/Project Coding and Data Files/Trained Models/23-03-2021 19:18:56/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QDEecGwGTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475aaddc-98d7-4ce9-bb79-1748bf08337a"
      },
      "source": [
        "# this part is for loading parameters and extracting features\n",
        "# load parameters\n",
        "parameter_version = \"23-03-2021 19:18:56\"\n",
        "if not os.path.exists(os.path.join(project_root, \"features\", parameter_version)):\n",
        "    os.mkdir(os.path.join(project_root, \"features\", parameter_version))\n",
        "\n",
        "my_model_pretrained = OriginCNN()\n",
        "my_model_pretrained.build()\n",
        "my_model_pretrained.load_model(os.path.join(project_root, \"Trained Models\", parameter_version))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,397\n",
            "Trainable params: 35,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dC4KT0HjTqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e7e0fa-f530-4152-edf2-bf191de684df"
      },
      "source": [
        "# load training data, extract features and save to npy files\n",
        "file_num = 9\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(True, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.train_images)\n",
        "  # feature = myModel.extract_feature(myData.train_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtrain_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtrain_feature%d.npy\" % i)\n",
        "  print(feature)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "# load testing data, extract features and save to npy files\n",
        "file_num = 2\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.test_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtest_feature%d.npy\" % i)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain_feature0.npy\n",
            "[[0.35830727 0.10113482 0.         ... 0.         0.         0.47119415]\n",
            " [0.35830727 0.04669811 0.         ... 0.         0.         0.200975  ]\n",
            " [0.47821516 0.11293279 0.         ... 0.         0.         0.31759244]\n",
            " ...\n",
            " [0.35830727 0.10821177 0.         ... 0.         0.         0.3621609 ]\n",
            " [0.38736483 0.         0.         ... 0.         0.         0.50423074]\n",
            " [0.35830727 0.12817381 0.         ... 0.         0.         0.16432361]]\n",
            "Xtrain_feature1.npy\n",
            "[[0.31001762 0.08447442 0.         ... 0.         0.         0.54034066]\n",
            " [0.45026395 0.04669811 0.         ... 0.         0.         0.62844735]\n",
            " [0.36011726 0.10609142 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.39272144 0.1103986  0.         ... 0.         0.         0.20643379]\n",
            " [0.37816614 0.12638164 0.         ... 0.         0.         0.36818033]\n",
            " [0.48419    0.46840778 0.         ... 0.         0.         1.2397174 ]]\n",
            "Xtrain_feature2.npy\n",
            "[[0.5399451  0.04669811 0.         ... 0.         0.         0.33048525]\n",
            " [0.60250944 0.18666117 0.         ... 0.         0.         0.40565366]\n",
            " [0.27334434 0.14828078 0.         ... 0.         0.         0.452514  ]\n",
            " ...\n",
            " [0.5731983  0.04669811 0.         ... 0.         0.         0.09627229]\n",
            " [0.47941777 0.47430038 0.         ... 0.         0.         0.23661497]\n",
            " [0.37783462 0.09423186 0.         ... 0.         0.         0.24922517]]\n",
            "Xtrain_feature3.npy\n",
            "[[0.32879612 0.         0.         ... 0.         0.         0.4694653 ]\n",
            " [0.6938508  0.05547374 0.         ... 0.         0.         0.91222274]\n",
            " [0.36964494 0.19902071 0.         ... 0.         0.         0.71168756]\n",
            " ...\n",
            " [0.4383259  0.15725595 0.         ... 0.         0.         0.4951031 ]\n",
            " [0.5663408  0.04669811 0.         ... 0.         0.         0.59657353]\n",
            " [0.3189979  0.26821998 0.02016739 ... 0.         0.         0.293995  ]]\n",
            "Xtrain_feature4.npy\n",
            "[[0.5482393  0.08710946 0.         ... 0.         0.         0.5158137 ]\n",
            " [0.35830727 0.09960653 0.         ... 0.         0.         0.54345   ]\n",
            " [0.47943452 0.12549722 0.         ... 0.         0.         0.5850111 ]\n",
            " ...\n",
            " [0.31712344 0.37991413 0.         ... 0.         0.         0.47146785]\n",
            " [0.3831841  0.08544667 0.         ... 0.         0.         0.7459256 ]\n",
            " [0.4939859  0.07166357 0.         ... 0.         0.         0.        ]]\n",
            "Xtrain_feature5.npy\n",
            "[[0.25912037 0.10348704 0.         ... 0.         0.         0.2797989 ]\n",
            " [0.56932783 0.053529   0.         ... 0.         0.         0.7505858 ]\n",
            " [0.34870118 0.04483921 0.         ... 0.         0.         0.547003  ]\n",
            " ...\n",
            " [0.50984347 0.11558133 0.         ... 0.         0.         0.43577933]\n",
            " [0.35830727 0.04669811 0.         ... 0.         0.         0.35837883]\n",
            " [0.35321733 0.04388131 0.         ... 0.         0.         0.63443315]]\n",
            "Xtrain_feature6.npy\n",
            "[[0.3554677  0.2251327  0.         ... 0.         0.         1.0805299 ]\n",
            " [0.34170717 0.10629591 0.         ... 0.         0.         0.16824785]\n",
            " [0.3612893  0.10474242 0.         ... 0.         0.         0.7805975 ]\n",
            " ...\n",
            " [0.5219863  0.22546814 0.         ... 0.         0.         0.7206606 ]\n",
            " [0.52372545 0.19352484 0.         ... 0.         0.         1.0042638 ]\n",
            " [0.47989467 0.11977142 0.         ... 0.         0.         0.89010364]]\n",
            "Xtrain_feature7.npy\n",
            "[[0.46209344 0.04669811 0.         ... 0.         0.         1.5074818 ]\n",
            " [0.35830727 0.14953604 0.         ... 0.         0.         0.7582077 ]\n",
            " [0.6095504  0.09464127 0.         ... 0.         0.         0.19994576]\n",
            " ...\n",
            " [0.8160008  0.06942797 0.         ... 0.         0.         0.40248483]\n",
            " [0.37412393 0.448774   0.         ... 0.         0.         0.27251655]\n",
            " [0.3598394  0.10610101 0.         ... 0.         0.         0.98659545]]\n",
            "Xtrain_feature8.npy\n",
            "[[0.39482647 0.11724132 0.         ... 0.         0.         0.4347384 ]\n",
            " [0.39845127 0.4381626  0.         ... 0.         0.         0.48718914]\n",
            " [0.63168937 0.25750208 0.         ... 0.         0.         1.0495282 ]\n",
            " ...\n",
            " [0.6317685  0.13022763 0.         ... 0.         0.         0.38409802]\n",
            " [0.59687954 0.17956656 0.         ... 0.         0.         1.5910499 ]\n",
            " [0.28515217 0.11274222 0.         ... 0.         0.         0.6627514 ]]\n",
            "Xtest_feature0.npy\n",
            "Xtest_feature1.npy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}