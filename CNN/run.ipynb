{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoAP5Dn3DwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, re, sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import skimage.io\n",
        "import gc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYucCZeH_dHH",
        "outputId": "ef9fe668-7c31-4349-9291-a749ac10082f"
      },
      "source": [
        "# test if GPUs are available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiMh4isCHnS",
        "outputId": "3d22a094-eb43-4785-b107-432a043395d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQvxwJFDmYk"
      },
      "source": [
        "# set project root, maybe you need to firstly \n",
        "# add shortcut of CS 766 Project to drive.\n",
        "project_root = './drive/MyDrive/CS 766 Project/Project Coding and Data Files'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJsefpJ-XjF"
      },
      "source": [
        "# class to initialize CNNs\n",
        "class OriginCNN(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = 'categorical_crossentropy'\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1000, 1000, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs):\n",
        "    self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-3].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3suc98v-bRj"
      },
      "source": [
        "class DR_resized(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id):\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # add a dimension for channels\n",
        "      all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "      \n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "        # load images\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # add a dimension for channels\n",
        "      all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOADsxqM3DwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b45ce9-1c08-400d-e0ff-fbf45cb13c01"
      },
      "source": [
        "# initialize CNN\n",
        "myModel = OriginCNN()\n",
        "myModel.build()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,397\n",
            "Trainable params: 35,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJnfPqP3DwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef13d21-34f6-47ef-965e-2a534013a5b1"
      },
      "source": [
        "# training process\n",
        "file_num = 9\n",
        "epoch_num = 5\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(True, i)\n",
        "  print(\"data batch %d loaded\" % i)\n",
        "  myModel.train(myData.train_images, myData.train_labels, epoch_num)\n",
        "  print(\"data batch %d trained\" % i)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data batch 0 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 99ms/step - loss: 1.6159 - accuracy: 0.1893 - val_loss: 2.1453 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5712 - accuracy: 0.2239 - val_loss: 2.1298 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5738 - accuracy: 0.2483 - val_loss: 2.0299 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 1.5570 - accuracy: 0.2830 - val_loss: 2.1751 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.5375 - accuracy: 0.3596 - val_loss: 2.2698 - val_accuracy: 0.0000e+00\n",
            "data batch 0 trained\n",
            "data batch 1 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 82ms/step - loss: 1.5834 - accuracy: 0.2222 - val_loss: 1.9909 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 81ms/step - loss: 1.5780 - accuracy: 0.2294 - val_loss: 2.0908 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5663 - accuracy: 0.3226 - val_loss: 2.2480 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.5588 - accuracy: 0.4014 - val_loss: 2.2934 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.5465 - accuracy: 0.5018 - val_loss: 2.0588 - val_accuracy: 0.0000e+00\n",
            "data batch 1 trained\n",
            "data batch 2 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 83ms/step - loss: 1.5864 - accuracy: 0.2294 - val_loss: 2.1665 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 1.5827 - accuracy: 0.2043 - val_loss: 2.1825 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5643 - accuracy: 0.3082 - val_loss: 2.2051 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 1.5536 - accuracy: 0.3943 - val_loss: 2.1820 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.5431 - accuracy: 0.4086 - val_loss: 2.1969 - val_accuracy: 0.0000e+00\n",
            "data batch 2 trained\n",
            "data batch 3 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 83ms/step - loss: 1.5856 - accuracy: 0.2222 - val_loss: 2.2391 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 1.5735 - accuracy: 0.2975 - val_loss: 2.1607 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5552 - accuracy: 0.3513 - val_loss: 2.1377 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5463 - accuracy: 0.3333 - val_loss: 2.1791 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.5268 - accuracy: 0.5520 - val_loss: 2.2143 - val_accuracy: 0.0000e+00\n",
            "data batch 3 trained\n",
            "data batch 4 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 1.5846 - accuracy: 0.2115 - val_loss: 2.2590 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.5564 - accuracy: 0.3190 - val_loss: 2.2053 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5449 - accuracy: 0.3154 - val_loss: 2.1777 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 1.5129 - accuracy: 0.3763 - val_loss: 2.2106 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.4918 - accuracy: 0.4588 - val_loss: 2.2660 - val_accuracy: 0.0000e+00\n",
            "data batch 4 trained\n",
            "data batch 5 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 1.5972 - accuracy: 0.2043 - val_loss: 2.0964 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5687 - accuracy: 0.2330 - val_loss: 2.0079 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 1.5394 - accuracy: 0.3943 - val_loss: 2.2227 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.5104 - accuracy: 0.4480 - val_loss: 2.1737 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.4716 - accuracy: 0.4480 - val_loss: 2.1691 - val_accuracy: 0.0000e+00\n",
            "data batch 5 trained\n",
            "data batch 6 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 1.5955 - accuracy: 0.2186 - val_loss: 2.3957 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.5648 - accuracy: 0.2330 - val_loss: 2.1323 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5243 - accuracy: 0.3835 - val_loss: 2.2700 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.4806 - accuracy: 0.4839 - val_loss: 2.1956 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.4330 - accuracy: 0.5305 - val_loss: 2.0162 - val_accuracy: 0.0000e+00\n",
            "data batch 6 trained\n",
            "data batch 7 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 82ms/step - loss: 1.6009 - accuracy: 0.2509 - val_loss: 2.2010 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 1.5408 - accuracy: 0.3369 - val_loss: 2.0953 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 1.4871 - accuracy: 0.3584 - val_loss: 2.0878 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 1.4105 - accuracy: 0.5556 - val_loss: 1.8183 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 1.3359 - accuracy: 0.5090 - val_loss: 3.0911 - val_accuracy: 0.0000e+00\n",
            "data batch 7 trained\n",
            "data batch 8 loaded\n",
            "Epoch 1/5\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 1.6409 - accuracy: 0.2366 - val_loss: 1.8599 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5580 - accuracy: 0.3369 - val_loss: 2.0598 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.4896 - accuracy: 0.3871 - val_loss: 2.2921 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 1.4375 - accuracy: 0.4731 - val_loss: 2.1997 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 1.3691 - accuracy: 0.5376 - val_loss: 2.4237 - val_accuracy: 0.0000e+00\n",
            "data batch 8 trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0svWpVPkiUsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d1bbb0-53c1-4566-c138-ea1b49056293"
      },
      "source": [
        "# testing process\n",
        "file_num = 2\n",
        "num_list = []\n",
        "acc_list = []\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i)\n",
        "  num, acc = myModel.evaluate(myData.test_images, myData.test_labels)\n",
        "  print(num, acc)\n",
        "  num_list.append(num)\n",
        "  acc_list.append(acc)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 - 0s - loss: 1.6752 - accuracy: 0.1968\n",
            "310 0.1967741996049881\n",
            "14/14 - 1s - loss: 1.6749 - accuracy: 0.2341\n",
            "440 0.2340909093618393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL9za1Dkqjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5661996f-e15e-4a34-8814-7f83053b7077"
      },
      "source": [
        "# compute overall accuracy\n",
        "num_list = np.array(num_list)\n",
        "acc_list = np.array(acc_list)\n",
        "acc_overall = (num_list * acc_list).sum() / num_list.sum()\n",
        "print(acc_overall)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.733040004587972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQpdk7vC25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687e7799-04ab-4f53-f38e-e295b7a56c72"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(\"date and time =\", dt_string)\n",
        "myModel.save_model(os.path.join(project_root, \"Trained Models\", dt_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date and time = 16-03-2021 01:13:01\n",
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/CS 766 Project/Project Coding and Data Files/Trained Models/16-03-2021 01:13:01/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QDEecGwGTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df61b508-7425-4ee4-cb0e-a10fba35724f"
      },
      "source": [
        "# this part is for loading parameters and extracting features\n",
        "# load parameters\n",
        "parameter_version = \"16-03-2021 01:13:01\"\n",
        "if not os.path.exists(os.path.join(project_root, \"features\", parameter_version)):\n",
        "    os.mkdir(os.path.join(project_root, \"features\", parameter_version))\n",
        "\n",
        "my_model_pretrained = OriginCNN()\n",
        "my_model_pretrained.build()\n",
        "my_model_pretrained.load_model(os.path.join(project_root, \"Trained Models\", parameter_version))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,397\n",
            "Trainable params: 35,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dC4KT0HjTqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57c80f6e-857a-40d9-8f78-f9dc8a9406fc"
      },
      "source": [
        "# load training data, extract features and save to npy files\n",
        "file_num = 9\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(True, i)\n",
        "  feature = my_model_pretrained.extract_feature(myData.train_images)\n",
        "  # feature = myModel.extract_feature(myData.train_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtrain_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtrain_feature%d.npy\" % i)\n",
        "  print(feature)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "# load testing data, extract features and save to npy files\n",
        "file_num = 2\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i)\n",
        "  feature = my_model_pretrained.extract_feature(myData.test_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtest_feature%d.npy\" % i)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain_feature0.npy\n",
            "[[0.13327868 0.20634948 0.90001553 ... 0.         0.         0.21805297]\n",
            " [0.01951008 0.37221256 0.7282312  ... 0.         0.         0.10665232]\n",
            " [0.17231773 0.41383412 0.7655909  ... 0.         0.         0.10913053]\n",
            " ...\n",
            " [0.02165465 0.37796298 0.7282312  ... 0.         0.         0.        ]\n",
            " [0.10473075 0.41896763 0.7282312  ... 0.         0.         0.02509846]\n",
            " [0.14702679 0.5428476  0.79617906 ... 0.         0.         0.08386298]]\n",
            "Xtrain_feature1.npy\n",
            "[[0.03505823 0.45131296 0.8865059  ... 0.         0.         0.        ]\n",
            " [0.02727759 0.3653634  0.76590776 ... 0.         0.         0.18827017]\n",
            " [0.15299256 0.4470656  0.804988   ... 0.         0.         0.24860455]\n",
            " ...\n",
            " [0.02606652 0.46200976 0.8601537  ... 0.         0.         0.07274511]\n",
            " [0.02449954 0.4146142  0.7317382  ... 0.         0.         0.07876874]\n",
            " [0.1174073  0.3688428  0.89758325 ... 0.         0.         0.25072953]]\n",
            "Xtrain_feature2.npy\n",
            "[[0.04790842 0.37480325 0.8757356  ... 0.         0.         0.08381602]\n",
            " [0.06563143 0.55775183 0.8794564  ... 0.         0.         0.06224801]\n",
            " [0.03737243 0.42280585 0.7788186  ... 0.         0.         0.24251759]\n",
            " ...\n",
            " [0.         0.24420321 0.8169858  ... 0.         0.         0.04203554]\n",
            " [0.00147727 0.35078108 0.7282312  ... 0.         0.         0.10617365]\n",
            " [0.06345616 0.56897646 0.7282312  ... 0.         0.         0.06794108]]\n",
            "Xtrain_feature3.npy\n",
            "[[0.         0.2746724  0.86641365 ... 0.         0.         0.12762876]\n",
            " [0.11537831 0.31021282 0.7976262  ... 0.         0.         0.05509717]\n",
            " [0.02623079 0.20330879 0.9833737  ... 0.         0.         0.07952939]\n",
            " ...\n",
            " [0.19741605 0.25555634 0.8764184  ... 0.         0.         0.09524773]\n",
            " [0.         0.10161287 0.9599542  ... 0.         0.         0.0396145 ]\n",
            " [0.2413747  0.28995925 0.82422936 ... 0.         0.         0.13909036]]\n",
            "Xtrain_feature4.npy\n",
            "[[0.01098567 0.27852693 0.839905   ... 0.         0.         0.11748109]\n",
            " [0.15396935 0.5105844  0.8001491  ... 0.         0.         0.02783499]\n",
            " [0.13770652 0.5175116  0.7282312  ... 0.         0.         0.01424683]\n",
            " ...\n",
            " [0.26119208 0.4567204  0.73212147 ... 0.         0.         0.09497473]\n",
            " [0.02647999 0.40578824 0.76878196 ... 0.         0.         0.08242796]\n",
            " [0.         0.15109727 0.9457481  ... 0.         0.         0.17957327]]\n",
            "Xtrain_feature5.npy\n",
            "[[0.15249446 0.44712362 0.7736464  ... 0.         0.         0.20693447]\n",
            " [0.01139235 0.39941972 0.74728733 ... 0.         0.         0.16321343]\n",
            " [0.06540224 0.19407618 0.9792504  ... 0.         0.         0.10289358]\n",
            " ...\n",
            " [0.05257259 0.4562211  0.7775552  ... 0.         0.         0.08687835]\n",
            " [0.         0.36150223 0.8617835  ... 0.         0.         0.06296682]\n",
            " [0.02299413 0.30683804 0.77787113 ... 0.         0.         0.03979008]]\n",
            "Xtrain_feature6.npy\n",
            "[[0.         0.2400566  0.9101279  ... 0.         0.         0.1296089 ]\n",
            " [0.20278744 0.36002216 0.95639265 ... 0.         0.         0.2553167 ]\n",
            " [0.0830652  0.53484243 0.7282312  ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.07520046 0.28477624 0.8661058  ... 0.         0.         0.13764676]\n",
            " [0.10905218 0.53059685 0.7282312  ... 0.         0.         0.10382795]\n",
            " [0.11176044 0.42099434 0.8457029  ... 0.         0.         0.06361912]]\n",
            "Xtrain_feature7.npy\n",
            "[[0.16433635 0.5334587  0.7372402  ... 0.         0.         0.07310019]\n",
            " [0.03188619 0.36136082 0.7282312  ... 0.         0.         0.07028522]\n",
            " [0.05001209 0.44243822 0.8304264  ... 0.         0.         0.2603341 ]\n",
            " ...\n",
            " [0.20627856 0.51529753 0.84451157 ... 0.         0.         0.10409553]\n",
            " [0.04906179 0.17845495 0.8124714  ... 0.         0.         0.03382446]\n",
            " [0.0398259  0.37969083 0.7282312  ... 0.         0.         0.05531844]]\n",
            "Xtrain_feature8.npy\n",
            "[[0.09954436 0.48106885 0.7282312  ... 0.         0.         0.25890434]\n",
            " [0.17755921 0.30582768 0.8040771  ... 0.         0.         0.05182563]\n",
            " [0.04395655 0.3936967  0.76642793 ... 0.         0.         0.0708991 ]\n",
            " ...\n",
            " [0.04972691 0.37506813 0.83176404 ... 0.         0.         0.12330843]\n",
            " [0.00749402 0.25563988 0.8112242  ... 0.         0.         0.        ]\n",
            " [0.00600669 0.34723473 0.96731466 ... 0.         0.         0.06998026]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-772231bf85d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmyData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDR_resized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model_pretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;31m# np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xtest_feature%d.npy\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_model_pretrained' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVUYy7cqazxp"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odec2bNGa3h8",
        "outputId": "b4d5614e-8bd9-4d3e-cb17-e0f449cd6868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "print(y_train.shape)\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "print(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "(60000,)\n",
            "[5 0 4 ... 5 6 8]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}