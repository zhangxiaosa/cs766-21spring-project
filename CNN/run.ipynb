{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoAP5Dn3DwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, re, sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import gc\n",
        "import skimage.transform\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYucCZeH_dHH",
        "outputId": "c5ceff1c-f764-4cbe-bf3a-0518c9da6815"
      },
      "source": [
        "# test if GPUs are available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiMh4isCHnS",
        "outputId": "7562120a-ba0b-4376-c9cc-e2417f8fdae5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQvxwJFDmYk"
      },
      "source": [
        "# set project root, maybe you need to firstly \n",
        "# add shortcut of CS 766 Project to drive.\n",
        "project_root = './drive/MyDrive/CS 766 Project/Project Coding and Data Files/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJsefpJ-XjF"
      },
      "source": [
        "# class to initialize CNNs\n",
        "class OriginCNN(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1000, 1000, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(2))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs, datagen=None):\n",
        "    if(datagen):\n",
        "      train_generator = datagen.flow(train_images, train_labels, batch_size=32, subset='training')\n",
        "      validation_generator = datagen.flow(train_images, train_labels, batch_size=32, subset='validation')\n",
        "      self.history = self.model.fit_generator(train_generator,\n",
        "                                              # steps_per_epoch=len(train_generator) / 32,\n",
        "                                              validation_data=validation_generator, \n",
        "                                              # validation_steps=len(validation_generator) / 32,\n",
        "                                              epochs=epochs, \n",
        "                                              shuffle=False)\n",
        "    else:\n",
        "      self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "    return self.history\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-3].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSiCn4VzdaV8"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50, InceptionV3, VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "class Res50(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    base_model = InceptionV3(\n",
        "      include_top=False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    # add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have n classes\n",
        "    predictions = Dense(2)(x)\n",
        "\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs, datagen=None):\n",
        "    if(datagen):\n",
        "      train_images = train_images / 255.0\n",
        "      train_generator = datagen.flow(train_images, train_labels, batch_size=32, subset='training')\n",
        "      validation_generator = datagen.flow(train_images, train_labels, batch_size=32, subset='validation')\n",
        "      self.history = self.model.fit_generator(train_generator,\n",
        "                                              # steps_per_epoch=len(train_generator) / 32,\n",
        "                                              validation_data=validation_generator, \n",
        "                                              # validation_steps=len(validation_generator) / 32,\n",
        "                                              epochs=epochs, \n",
        "                                              shuffle=False)\n",
        "    else:\n",
        "      self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "    return self.history\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-2].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3suc98v-bRj"
      },
      "source": [
        "class DR_resized(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate, size=None):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Preprocessed_Even_Class_Distribution_Datasets\", \"04_Only_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"Xtrain\", \"04_Only_Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # resize\n",
        "      # gray scale\n",
        "      if(size):\n",
        "        if (len(all_images[0].shape) == 2):\n",
        "          all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 3));\n",
        "          for i in range(all_images.shape[0]):\n",
        "            img = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]))\n",
        "            img3 = np.repeat(img, 3, axis=2)\n",
        "            all_images_resize[i] = img3\n",
        "        # color image\n",
        "        else:\n",
        "          all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 3));\n",
        "          for i in range(all_images.shape[0]):\n",
        "            all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]))\n",
        "        all_images = all_images_resize\n",
        "\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Preprocessed_Even_Class_Distribution_Datasets\", \"04_Only_Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"Ytrain\", \"04_Only_Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "      \n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      np.random.seed(0);\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Preprocessed_Even_Class_Distribution_Datasets\", \"04_Only_Processed_Xest_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"Xtest\", \"04_Only_Color_Processed_Xest_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      if (len(all_images[0].shape) == 2):\n",
        "        all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # resize\n",
        "      if(size):\n",
        "        # gray scale\n",
        "        if (len(all_images[0].shape) == 2):\n",
        "          all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 1));\n",
        "          for i in range(all_images.shape[0]):\n",
        "            all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "        # color image\n",
        "        else:\n",
        "          all_images_resize = np.zeros((all_images.shape[0], size[0], size[1], 3));\n",
        "          for i in range(all_images.shape[0]):\n",
        "            all_images_resize[i] = skimage.transform.resize(all_images[i].astype('float64'), (size[0], size[1]));\n",
        "        all_images = all_images_resize\n",
        "\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Preprocessed_Even_Class_Distribution_Datasets\", \"04_Only_Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"04_Only_Color_Even_Class_Distribution_Datasets\", \"Ytest\", \"04_Only_Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L57sy6Z02c-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Cifar(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      # if (len(all_images[0].shape) == 2):\n",
        "        # all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "      train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "      all_images = train_images\n",
        "      all_labels = train_labels\n",
        "      \n",
        "      # one hot\n",
        "      # all_labels_one_hot = np.zeros((all_labels.size, 10))\n",
        "      # all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      all_labels_one_hot = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "      # load images\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Color_Even_Class_Distribution_Datasets\", \"Color_Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      \n",
        "      # add a dimension for channels if gray scale images\n",
        "      # if (len(all_images[0].shape) == 2):\n",
        "      #   all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "      train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "      all_images = test_images\n",
        "      all_labels = test_labels\n",
        "\n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 10))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOADsxqM3DwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac794fd5-54e3-4c3d-d98e-8331c55b8bfb"
      },
      "source": [
        "# initialize CNN\n",
        "# myModel = OriginCNN()\n",
        "myModel = Res50()\n",
        "myModel.build()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 111, 111, 32) 864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 111, 111, 32) 96          conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 111, 111, 32) 0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 109, 109, 32) 9216        activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 109, 109, 32) 96          conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 109, 109, 32) 0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 109, 109, 64) 18432       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 109, 109, 64) 192         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 109, 109, 64) 0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 54, 54, 64)   0           activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 54, 54, 80)   5120        max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 54, 54, 80)   240         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 54, 54, 80)   0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 52, 52, 192)  138240      activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 52, 52, 192)  576         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 52, 52, 192)  0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 25, 25, 192)  0           activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 25, 25, 64)   192         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 25, 25, 64)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 25, 25, 48)   9216        max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 25, 25, 96)   55296       activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 25, 25, 48)   144         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 25, 25, 96)   288         conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 25, 25, 48)   0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 25, 25, 96)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 25, 25, 192)  0           max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 25, 25, 64)   12288       max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 25, 25, 64)   76800       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 25, 25, 96)   82944       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 25, 25, 32)   6144        average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 25, 25, 64)   192         conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 25, 25, 64)   192         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 25, 25, 96)   288         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 25, 25, 32)   96          conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 25, 25, 64)   0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 25, 25, 64)   0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 25, 25, 96)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 25, 25, 32)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_193[0][0]             \n",
            "                                                                 activation_195[0][0]             \n",
            "                                                                 activation_198[0][0]             \n",
            "                                                                 activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 25, 25, 64)   192         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 25, 25, 64)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 25, 25, 96)   55296       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 25, 25, 48)   144         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 25, 25, 96)   288         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 25, 25, 48)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 25, 25, 96)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 25, 25, 64)   76800       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 25, 25, 96)   82944       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 25, 25, 64)   16384       average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 25, 25, 64)   192         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 25, 25, 64)   192         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 25, 25, 96)   288         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 25, 25, 64)   192         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 25, 25, 64)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 25, 25, 64)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 25, 25, 96)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 25, 25, 64)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_200[0][0]             \n",
            "                                                                 activation_202[0][0]             \n",
            "                                                                 activation_205[0][0]             \n",
            "                                                                 activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 25, 25, 64)   192         conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 25, 25, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 25, 25, 96)   55296       activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 25, 25, 48)   144         conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 25, 25, 96)   288         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 25, 25, 48)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 25, 25, 96)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 25, 25, 64)   76800       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 25, 25, 96)   82944       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 25, 25, 64)   18432       average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 25, 25, 64)   192         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 25, 25, 64)   192         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 25, 25, 96)   288         conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 25, 25, 64)   192         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 25, 25, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 25, 25, 64)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 25, 25, 96)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 25, 25, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_207[0][0]             \n",
            "                                                                 activation_209[0][0]             \n",
            "                                                                 activation_212[0][0]             \n",
            "                                                                 activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 25, 25, 64)   192         conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 25, 25, 64)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 25, 25, 96)   55296       activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 25, 25, 96)   288         conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 25, 25, 96)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 12, 12, 96)   82944       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 12, 12, 384)  1152        conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 12, 12, 96)   288         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 12, 12, 384)  0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 12, 12, 96)   0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_214[0][0]             \n",
            "                                                                 activation_217[0][0]             \n",
            "                                                                 max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 12, 12, 128)  384         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 12, 12, 128)  0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 12, 12, 128)  114688      activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 12, 12, 128)  384         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 12, 12, 128)  0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 12, 12, 128)  114688      activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 12, 12, 128)  384         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 12, 12, 128)  384         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 12, 12, 128)  0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 12, 12, 128)  0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 12, 12, 128)  114688      activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 12, 12, 128)  114688      activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 12, 12, 128)  384         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 12, 12, 128)  384         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 12, 12, 128)  0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 12, 12, 128)  0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_21 (AveragePo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 12, 12, 192)  172032      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 12, 12, 192)  172032      activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 12, 12, 192)  576         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 12, 12, 192)  576         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 12, 12, 192)  576         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 12, 12, 192)  576         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 12, 12, 192)  0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 12, 12, 192)  0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 12, 12, 192)  0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 12, 12, 192)  0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_218[0][0]             \n",
            "                                                                 activation_221[0][0]             \n",
            "                                                                 activation_226[0][0]             \n",
            "                                                                 activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 12, 12, 160)  480         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 12, 12, 160)  0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 12, 12, 160)  179200      activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 12, 12, 160)  480         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 12, 12, 160)  0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 12, 12, 160)  179200      activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 12, 12, 160)  480         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 12, 12, 160)  480         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 12, 12, 160)  0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 12, 12, 160)  0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 12, 12, 160)  179200      activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 12, 12, 160)  179200      activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 12, 12, 160)  480         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 12, 12, 160)  480         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 12, 12, 160)  0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 12, 12, 160)  0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_22 (AveragePo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 12, 12, 192)  215040      activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 12, 12, 192)  215040      activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 12, 12, 192)  576         conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 12, 12, 192)  576         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 12, 12, 192)  576         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 12, 12, 192)  576         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 12, 12, 192)  0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 12, 12, 192)  0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 12, 12, 192)  0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 12, 12, 192)  0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_228[0][0]             \n",
            "                                                                 activation_231[0][0]             \n",
            "                                                                 activation_236[0][0]             \n",
            "                                                                 activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 12, 12, 160)  480         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 12, 12, 160)  0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 12, 12, 160)  179200      activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 12, 12, 160)  480         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 12, 12, 160)  0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 12, 12, 160)  179200      activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 12, 12, 160)  480         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 12, 12, 160)  480         conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 12, 12, 160)  0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 12, 12, 160)  0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 12, 12, 160)  179200      activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 12, 12, 160)  179200      activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 12, 12, 160)  480         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 12, 12, 160)  480         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 12, 12, 160)  0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 12, 12, 160)  0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 12, 12, 192)  215040      activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 12, 12, 192)  215040      activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 12, 12, 192)  576         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 12, 12, 192)  576         conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 12, 12, 192)  576         conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 12, 12, 192)  576         conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 12, 12, 192)  0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 12, 12, 192)  0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 12, 12, 192)  0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 12, 12, 192)  0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_238[0][0]             \n",
            "                                                                 activation_241[0][0]             \n",
            "                                                                 activation_246[0][0]             \n",
            "                                                                 activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 12, 12, 192)  576         conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 12, 12, 192)  0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 12, 12, 192)  258048      activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 12, 12, 192)  576         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 12, 12, 192)  0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 12, 12, 192)  258048      activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 12, 12, 192)  576         conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 12, 12, 192)  576         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 12, 12, 192)  0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 12, 12, 192)  0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 12, 12, 192)  258048      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 12, 12, 192)  258048      activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 12, 12, 192)  576         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 12, 12, 192)  576         conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 12, 12, 192)  0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 12, 12, 192)  0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 12, 12, 192)  258048      activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 12, 12, 192)  258048      activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 12, 12, 192)  147456      average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 12, 12, 192)  576         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 12, 12, 192)  576         conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 12, 12, 192)  576         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 12, 12, 192)  576         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 12, 12, 192)  0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 12, 12, 192)  0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 12, 12, 192)  0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 12, 12, 192)  0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_248[0][0]             \n",
            "                                                                 activation_251[0][0]             \n",
            "                                                                 activation_256[0][0]             \n",
            "                                                                 activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 12, 12, 192)  576         conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 12, 12, 192)  0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 12, 12, 192)  258048      activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 12, 12, 192)  576         conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 12, 12, 192)  0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 12, 12, 192)  258048      activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 12, 12, 192)  576         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 12, 12, 192)  576         conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 12, 12, 192)  0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 12, 12, 192)  0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 5, 5, 320)    552960      activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 5, 5, 192)    331776      activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 5, 5, 320)    960         conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 5, 5, 192)    576         conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 5, 5, 320)    0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 5, 5, 192)    0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling2D) (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_259[0][0]             \n",
            "                                                                 activation_263[0][0]             \n",
            "                                                                 max_pooling2d_17[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 5, 5, 448)    1344        conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 5, 5, 448)    0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 5, 5, 384)    1548288     activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 5, 5, 384)    1152        conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 5, 5, 384)    1152        conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 5, 5, 384)    0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 5, 5, 384)    0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 5, 5, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 5, 5, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 5, 5, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 5, 5, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 5, 5, 384)    1152        conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 5, 5, 384)    1152        conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 5, 5, 384)    1152        conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 5, 5, 384)    1152        conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 5, 5, 192)    245760      average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 5, 5, 320)    960         conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 5, 5, 384)    0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 5, 5, 384)    0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 5, 5, 384)    0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 5, 5, 384)    0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 5, 5, 192)    576         conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 5, 5, 320)    0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_266[0][0]             \n",
            "                                                                 activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 5, 5, 768)    0           activation_270[0][0]             \n",
            "                                                                 activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 5, 5, 192)    0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_264[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_4[0][0]              \n",
            "                                                                 activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_283 (Conv2D)             (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 5, 5, 448)    1344        conv2d_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 5, 5, 448)    0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_284 (Conv2D)             (None, 5, 5, 384)    1548288     activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 5, 5, 384)    1152        conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 5, 5, 384)    1152        conv2d_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 5, 5, 384)    0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 5, 5, 384)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 5, 5, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_282 (Conv2D)             (None, 5, 5, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_285 (Conv2D)             (None, 5, 5, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_286 (Conv2D)             (None, 5, 5, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 5, 5, 384)    1152        conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 5, 5, 384)    1152        conv2d_282[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 5, 5, 384)    1152        conv2d_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 5, 5, 384)    1152        conv2d_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_287 (Conv2D)             (None, 5, 5, 192)    393216      average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 5, 5, 320)    960         conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 5, 5, 384)    0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 5, 5, 384)    0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 5, 5, 384)    0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 5, 5, 384)    0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 5, 5, 192)    576         conv2d_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 5, 5, 320)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_275[0][0]             \n",
            "                                                                 activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 5, 5, 768)    0           activation_279[0][0]             \n",
            "                                                                 activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 5, 5, 192)    0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_273[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_5[0][0]              \n",
            "                                                                 activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          1049088     global_average_pooling2d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 2)            1026        dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,852,898\n",
            "Trainable params: 22,818,466\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJnfPqP3DwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb0a8bb-6cee-44e2-d477-22730e82c5cd"
      },
      "source": [
        "# training process\n",
        "datagen = ImageDataGenerator(        \n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.1,  \n",
        "            height_shift_range=0.1,    \n",
        "            shear_range=0.1,        \n",
        "            zoom_range=0.1,        \n",
        "            horizontal_flip=True,         \n",
        "            fill_mode='constant', cval=0,\n",
        "            channel_shift_range=20,\n",
        "            brightness_range=[0.5,1.5],\n",
        "            validation_split=0.1,\n",
        "            rescale=1.0/255\n",
        "            )\n",
        "file_num = 9\n",
        "epoch_num = 30\n",
        "history_array = np.zeros((file_num, epoch_num, 4))\n",
        "for epoch in range(epoch_num):\n",
        "  print(\"\\n\\nstart epoch %d\" % epoch)\n",
        "  for i in range(file_num):\n",
        "    # load data\n",
        "    myData = DR_resized(True, i, 32, 0.1, (224, 224))\n",
        "    # myData = DR_resized(True, i, 32, 0.1)\n",
        "    print(\"data batch %d loaded\" % i)\n",
        "\n",
        "    history = myModel.train(myData.train_images, myData.train_labels, epochs=1)\n",
        "    # history = myModel.train(myData.train_images, myData.train_labels, epochs=1, datagen=datagen)\n",
        "    history_array[i, epoch, 0] = history.history['loss'][0]\n",
        "    history_array[i, epoch, 1] = history.history['accuracy'][0]\n",
        "    history_array[i, epoch, 2] = history.history['val_loss'][0]\n",
        "    history_array[i, epoch, 3] = history.history['val_accuracy'][0]\n",
        "    myData.clear()\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "start epoch 0\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 11s 1s/step - loss: 0.9627 - accuracy: 0.4551 - val_loss: 34.9457 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 1.9826 - accuracy: 0.4775 - val_loss: 36261.0234 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 520ms/step - loss: 1.0905 - accuracy: 0.5045 - val_loss: 26442.1211 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 2.1377 - accuracy: 0.4775 - val_loss: 92953.7578 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.8382 - accuracy: 0.5495 - val_loss: 960115.6875 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 532ms/step - loss: 1.0982 - accuracy: 0.5135 - val_loss: 596613.2500 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.7675 - accuracy: 0.5045 - val_loss: 75901.1094 - val_accuracy: 0.6154\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 1.0849 - accuracy: 0.5225 - val_loss: 35467.4492 - val_accuracy: 0.6154\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7462 - accuracy: 0.5135 - val_loss: 110878.1328 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 1\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7448 - accuracy: 0.5135 - val_loss: 63649.6836 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.7365 - accuracy: 0.5405 - val_loss: 16637.0039 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 521ms/step - loss: 0.7671 - accuracy: 0.4414 - val_loss: 5052.8872 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.7403 - accuracy: 0.4775 - val_loss: 2749.2249 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7159 - accuracy: 0.5135 - val_loss: 705.1897 - val_accuracy: 0.6154\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 533ms/step - loss: 0.7109 - accuracy: 0.5135 - val_loss: 39.8723 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.6991 - accuracy: 0.5225 - val_loss: 244.6027 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.6945 - accuracy: 0.5315 - val_loss: 530.1335 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7300 - accuracy: 0.4775 - val_loss: 98.1594 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 2\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.6749 - accuracy: 0.5135 - val_loss: 0.7804 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7318 - accuracy: 0.5135 - val_loss: 0.7378 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6959 - accuracy: 0.4775 - val_loss: 0.7106 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.7000 - accuracy: 0.4505 - val_loss: 0.7146 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6888 - accuracy: 0.5045 - val_loss: 0.7397 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7497 - accuracy: 0.5135 - val_loss: 0.7351 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.7573 - accuracy: 0.5856 - val_loss: 0.7144 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6790 - accuracy: 0.4865 - val_loss: 0.7041 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6884 - accuracy: 0.5045 - val_loss: 0.6953 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 3\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6906 - accuracy: 0.5045 - val_loss: 0.7018 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.7210 - accuracy: 0.5045 - val_loss: 0.7018 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6933 - accuracy: 0.5766 - val_loss: 0.7045 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.7118 - accuracy: 0.4595 - val_loss: 0.6970 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7056 - accuracy: 0.5045 - val_loss: 0.6967 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6999 - accuracy: 0.4414 - val_loss: 0.7019 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.7140 - accuracy: 0.4685 - val_loss: 0.6966 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6914 - accuracy: 0.4955 - val_loss: 0.6941 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6859 - accuracy: 0.5135 - val_loss: 0.6958 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 4\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6735 - accuracy: 0.5405 - val_loss: 0.7013 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.7394 - accuracy: 0.4685 - val_loss: 0.6972 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.7270 - accuracy: 0.5225 - val_loss: 0.6906 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.7072 - accuracy: 0.4865 - val_loss: 0.6897 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7418 - accuracy: 0.4865 - val_loss: 0.6966 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7141 - accuracy: 0.4505 - val_loss: 0.6969 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7006 - accuracy: 0.5045 - val_loss: 0.6973 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.6833 - accuracy: 0.5405 - val_loss: 0.6959 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6865 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 5\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6917 - accuracy: 0.5135 - val_loss: 0.6942 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.6944 - accuracy: 0.4955 - val_loss: 0.6959 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.6808 - accuracy: 0.4685 - val_loss: 0.6924 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6967 - accuracy: 0.4775 - val_loss: 0.6891 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.7046 - accuracy: 0.4865 - val_loss: 0.6917 - val_accuracy: 0.6154\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7316 - accuracy: 0.5586 - val_loss: 0.7034 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.7644 - accuracy: 0.5045 - val_loss: 0.7155 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.6940 - accuracy: 0.5135 - val_loss: 0.7106 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6914 - accuracy: 0.5045 - val_loss: 0.7044 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 6\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6946 - accuracy: 0.4685 - val_loss: 0.7021 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.7186 - accuracy: 0.5676 - val_loss: 0.7074 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6695 - accuracy: 0.5135 - val_loss: 0.7132 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.8057 - accuracy: 0.5135 - val_loss: 0.7115 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6776 - accuracy: 0.5135 - val_loss: 0.7056 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.7019 - accuracy: 0.5315 - val_loss: 0.7037 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6937 - accuracy: 0.5135 - val_loss: 0.7058 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.6835 - accuracy: 0.5135 - val_loss: 0.7062 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.6867 - accuracy: 0.5135 - val_loss: 0.7064 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 7\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6732 - accuracy: 0.5135 - val_loss: 0.7031 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 535ms/step - loss: 0.6999 - accuracy: 0.5135 - val_loss: 0.7023 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6768 - accuracy: 0.5045 - val_loss: 0.6987 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.7116 - accuracy: 0.5225 - val_loss: 0.6984 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.6968 - accuracy: 0.4955 - val_loss: 0.7028 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.7190 - accuracy: 0.3964 - val_loss: 0.7044 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.7154 - accuracy: 0.5045 - val_loss: 0.7071 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.7090 - accuracy: 0.4955 - val_loss: 0.7066 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6826 - accuracy: 0.5135 - val_loss: 0.7013 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 8\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6838 - accuracy: 0.4775 - val_loss: 0.6985 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7295 - accuracy: 0.4865 - val_loss: 0.6993 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.7332 - accuracy: 0.4685 - val_loss: 0.7056 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.7077 - accuracy: 0.4865 - val_loss: 0.7106 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.6919 - accuracy: 0.5045 - val_loss: 0.7156 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.7055 - accuracy: 0.5135 - val_loss: 0.7151 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 531ms/step - loss: 0.7005 - accuracy: 0.5045 - val_loss: 0.7079 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.6962 - accuracy: 0.5405 - val_loss: 0.7052 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.6907 - accuracy: 0.4865 - val_loss: 0.7078 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 9\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6880 - accuracy: 0.5135 - val_loss: 0.7106 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.6816 - accuracy: 0.5135 - val_loss: 0.7058 - val_accuracy: 0.3846\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6764 - accuracy: 0.5225 - val_loss: 0.6427 - val_accuracy: 0.3846\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.7329 - accuracy: 0.4775 - val_loss: 0.6897 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6897 - accuracy: 0.5135 - val_loss: 0.6903 - val_accuracy: 0.6154\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.7198 - accuracy: 0.4865 - val_loss: 0.7133 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.7064 - accuracy: 0.4775 - val_loss: 0.7100 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 530ms/step - loss: 0.7181 - accuracy: 0.5135 - val_loss: 0.7057 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6843 - accuracy: 0.4685 - val_loss: 0.7018 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 10\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6926 - accuracy: 0.5045 - val_loss: 0.7006 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6918 - accuracy: 0.4324 - val_loss: 0.6930 - val_accuracy: 0.6154\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.7050 - accuracy: 0.4955 - val_loss: 0.6898 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.7318 - accuracy: 0.4775 - val_loss: 0.6985 - val_accuracy: 0.3846\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 524ms/step - loss: 0.6972 - accuracy: 0.4234 - val_loss: 0.7094 - val_accuracy: 0.3846\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 529ms/step - loss: 0.6972 - accuracy: 0.5135 - val_loss: 0.7108 - val_accuracy: 0.3846\n",
            "data batch 6 loaded\n",
            "4/4 [==============================] - 2s 525ms/step - loss: 0.6817 - accuracy: 0.5135 - val_loss: 0.7107 - val_accuracy: 0.3846\n",
            "data batch 7 loaded\n",
            "4/4 [==============================] - 2s 522ms/step - loss: 0.6856 - accuracy: 0.5135 - val_loss: 0.7107 - val_accuracy: 0.3846\n",
            "data batch 8 loaded\n",
            "4/4 [==============================] - 2s 526ms/step - loss: 0.6814 - accuracy: 0.5135 - val_loss: 0.7069 - val_accuracy: 0.3846\n",
            "\n",
            "\n",
            "start epoch 11\n",
            "data batch 0 loaded\n",
            "4/4 [==============================] - 2s 527ms/step - loss: 0.6700 - accuracy: 0.5135 - val_loss: 0.7016 - val_accuracy: 0.3846\n",
            "data batch 1 loaded\n",
            "4/4 [==============================] - 2s 528ms/step - loss: 0.7111 - accuracy: 0.4505 - val_loss: 0.6925 - val_accuracy: 0.6154\n",
            "data batch 2 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.6756 - accuracy: 0.5135 - val_loss: 0.6877 - val_accuracy: 0.6154\n",
            "data batch 3 loaded\n",
            "4/4 [==============================] - 2s 519ms/step - loss: 0.7211 - accuracy: 0.4775 - val_loss: 0.6866 - val_accuracy: 0.6154\n",
            "data batch 4 loaded\n",
            "4/4 [==============================] - 2s 516ms/step - loss: 0.6882 - accuracy: 0.5135 - val_loss: 0.6863 - val_accuracy: 0.6154\n",
            "data batch 5 loaded\n",
            "4/4 [==============================] - 2s 523ms/step - loss: 0.7054 - accuracy: 0.4865 - val_loss: 0.7861 - val_accuracy: 0.5385\n",
            "data batch 6 loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0svWpVPkiUsK"
      },
      "source": [
        "# testing process\n",
        "file_num = 2\n",
        "num_list = []\n",
        "acc_list = []\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "for i in range(file_num):\n",
        "  # myData = DR_resized(False, i, 32, 0.1, (224, 224))\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  num, acc = myModel.evaluate(myData.test_images, myData.test_labels)\n",
        "  print(num, acc)\n",
        "  # p = myModel.model.predict(myData.test_images)\n",
        "  # print(p)\n",
        "  # m.update_state(p, myData.test_labels)\n",
        "  # num_list.append(num)\n",
        "  # acc_list.append(acc)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "print(m.result().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL9za1Dkqjy"
      },
      "source": [
        "# compute overall accuracy\n",
        "num_list = np.array(num_list)\n",
        "acc_list = np.array(acc_list)\n",
        "acc_overall = (num_list * acc_list).sum() / num_list.sum()\n",
        "print(acc_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQpdk7vC25"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(\"date and time =\", dt_string)\n",
        "myModel.save_model(os.path.join(project_root, \"Trained Models\", dt_string))\n",
        "np.save(os.path.join(project_root, \"Trained Models\", dt_string, \"history\"), history_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QDEecGwGTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531a80d5-dd44-480f-e208-c0af7d64d5df"
      },
      "source": [
        "# this part is for loading parameters and extracting features\n",
        "# load parameters\n",
        "parameter_version = \"21-04-2021 15:09:26\"\n",
        "if not os.path.exists(os.path.join(project_root, \"features\", parameter_version)):\n",
        "    os.mkdir(os.path.join(project_root, \"features\", parameter_version))\n",
        "\n",
        "my_model_pretrained = OriginCNN()\n",
        "# my_model_pretrained = Res50()\n",
        "my_model_pretrained.build()\n",
        "my_model_pretrained.load_model(os.path.join(project_root, \"Trained Models\", parameter_version))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 35,298\n",
            "Trainable params: 35,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dC4KT0HjTqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b0e0f8-9637-4a4c-f6e8-8d3f5d59c044"
      },
      "source": [
        "# load training data, extract features and save to npy files\n",
        "file_num = 9\n",
        "for i in range(file_num):\n",
        "  # myData = DR_resized(True, i, 32, 0.1, (224, 224))\n",
        "  myData = DR_resized(True, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.train_images)\n",
        "  # feature = myModel.extract_feature(myData.train_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtrain_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtrain_feature%d.npy\" % i)\n",
        "  print(feature.shape)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "# load testing data, extract features and save to npy files\n",
        "file_num = 2\n",
        "for i in range(file_num):\n",
        "  # myData = DR_resized(False, i, 32, 0.1, (224, 224))\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.test_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtest_feature%d.npy\" % i)\n",
        "  print(feature.shape)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain_feature0.npy\n",
            "(124, 512)\n",
            "Xtrain_feature1.npy\n",
            "(124, 512)\n",
            "Xtrain_feature2.npy\n",
            "(124, 512)\n",
            "Xtrain_feature3.npy\n",
            "(124, 512)\n",
            "Xtrain_feature4.npy\n",
            "(124, 512)\n",
            "Xtrain_feature5.npy\n",
            "(124, 512)\n",
            "Xtrain_feature6.npy\n",
            "(124, 512)\n",
            "Xtrain_feature7.npy\n",
            "(124, 512)\n",
            "Xtrain_feature8.npy\n",
            "(124, 512)\n",
            "Xtest_feature0.npy\n",
            "Xtest_feature1.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDhmQ8B9g22v"
      },
      "source": [
        "# confusion matrix\n",
        "file_num = 2\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "for i in range(file_num):\n",
        "  # myData = DR_resized(False, i, 32, 0.1, (224, 224))\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  y_true_list.append(myData.test_labels)\n",
        "  y_pred = myModel.predict(myData.test_images)\n",
        "  y_pred_list.append(y_pred)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6eXXdm4jgCS"
      },
      "source": [
        "print(y_true_list)\n",
        "print(y_pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}