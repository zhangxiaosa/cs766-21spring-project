{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGoAP5Dn3DwQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, re, sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYucCZeH_dHH",
        "outputId": "cbf2914a-b492-4f59-85fa-b7a0d0b6cd17"
      },
      "source": [
        "# test if GPUs are available\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiMh4isCHnS",
        "outputId": "1670e2a8-503b-4bb4-c992-205659c3ff54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJQvxwJFDmYk"
      },
      "source": [
        "# set project root, maybe you need to firstly \n",
        "# add shortcut of CS 766 Project to drive.\n",
        "project_root = './drive/MyDrive/CS 766 Project/Project Coding and Data Files'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJsefpJ-XjF"
      },
      "source": [
        "# class to initialize CNNs\n",
        "class OriginCNN(object):\n",
        "  \"\"\"docstring for OriginCNN\"\"\"\n",
        "  def __init__(self):\n",
        "    self.optimizer = 'adam'\n",
        "    # self.optimizer = tf.keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.loss = 'categorical_crossentropy'\n",
        "    # will add more properties\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1000, 1000, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(3,3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=self.loss,\n",
        "                metrics=['accuracy'])\n",
        "    self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_images, train_labels, epochs):\n",
        "    self.history = self.model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "  def train_on_batch(self, train_images, train_labels):\n",
        "    return self.model.train_on_batch(train_images, train_labels)\n",
        "\n",
        "  def predict(self, test_images):\n",
        "    return self.model.predict(test_images)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_images, test_labels):\n",
        "    _, test_acc = self.model.evaluate(test_images, test_labels, verbose=2)\n",
        "    return test_images.shape[0], test_acc\n",
        "\n",
        "  def save_model(self, filepath):\n",
        "    self.model.save(filepath)\n",
        "\n",
        "  def load_model(self, filepath):\n",
        "    self.model = tf.keras.models.load_model(filepath)\n",
        "\n",
        "  def extract_feature(self, images):\n",
        "    extract = models.Model(self.model.inputs, self.model.layers[-3].output)\n",
        "    return extract.predict(images)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3suc98v-bRj"
      },
      "source": [
        "class DR_resized(object):\n",
        "  \"\"\"docstring for DR_resized\"\"\"\n",
        "  def __init__(self, is_training, batch_id, batch_size, train_val_split_rate):\n",
        "    self.batch_size = batch_size\n",
        "    self.train_val_split_rate = train_val_split_rate\n",
        "    if(is_training):\n",
        "      # load images\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xtrain_Batch%d_even.npy\" % batch_id))\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\n",
        "      # add a dimension for channels\n",
        "      all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytrain\", \"Ytrain_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytrain\", \"Ytrain_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "      \n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "\n",
        "      # shuffle\n",
        "      indices = np.arange(all_images.shape[0])\n",
        "      np.random.shuffle(indices)\n",
        "      all_images = all_images[indices]\n",
        "      all_labels_one_hot = all_labels_one_hot[indices]\n",
        "\n",
        "      self.train_images = all_images\n",
        "      self.train_labels = all_labels_one_hot\n",
        "    \n",
        "    else:\n",
        "        # load images\n",
        "      all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Even_Class_Distribution_Datasets\", \"Processed_Xest_Batch%d_even.npy\") % batch_id)\n",
        "      # all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtest_Batch%d.npy\") % batch_id)\n",
        "      # add a dimension for channels\n",
        "      all_images = np.expand_dims(all_images, axis=-1)\n",
        "\n",
        "      # load labels\n",
        "      all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Even Class Distribution Datasets\", \"Ytest\", \"Yest_Batch%d_even.npy\" % batch_id), allow_pickle=True)\n",
        "      # all_labels = np.load(os.path.join(project_root, \"Data Batches\", \"Stratified Random Sampling Datasets\", \"Ytest\", \"Ytest_Batch%d.npy\" % batch_id), allow_pickle=True)\n",
        "      all_labels = np.array([item[1] for item in all_labels])\n",
        "\n",
        "      # one hot\n",
        "      all_labels_one_hot = np.zeros((all_labels.size, 5))\n",
        "      all_labels_one_hot[np.arange(all_labels.size),all_labels] = 1\n",
        "\n",
        "      # without one hot\n",
        "      # all_labels = np.expand_dims(all_labels, axis=-1)\n",
        "      \n",
        "      self.test_images = all_images\n",
        "      self.test_labels = all_labels_one_hot\n",
        "  \n",
        "  def clear(self):\n",
        "    self.train_images = None\n",
        "    self.validate_images = None\n",
        "    self.test_images = None\n",
        "    self.train_labels = None\n",
        "    self.validate_labels = None\n",
        "    self.test_labels = None\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOADsxqM3DwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4606c039-087f-4238-f4b5-2067557aa701"
      },
      "source": [
        "# initialize CNN\n",
        "myModel = OriginCNN()\n",
        "myModel.build()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,397\n",
            "Trainable params: 35,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJnfPqP3DwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05c1d570-e8d5-4cdd-85b2-ed180ad25a35"
      },
      "source": [
        "# training process\n",
        "file_num = 9\n",
        "epoch_num = 5\n",
        "for i in range(file_num):\n",
        "  # load data\n",
        "  myData = DR_resized(True, i, 32, 0.1)\n",
        "  print(\"\\n\\ndata batch %d loaded\" % i)\n",
        "\n",
        "  for j in range(epoch_num):\n",
        "    print(\"Start of epoch %d\" % j)\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "    # matric\n",
        "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    # train val split\n",
        "    val_num = round(myData.train_val_split_rate * myData.train_images.shape[0]);\n",
        "    # train dataset\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((myData.train_images[:-1 * val_num], myData.train_labels[:-1 * val_num]))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(myData.batch_size)\n",
        "    # val dataset\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((myData.train_images[-1 * val_num:], myData.train_labels[-1 * val_num:]))\n",
        "    val_dataset = val_dataset.shuffle(buffer_size=1024).batch(myData.batch_size)\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        loss_value = myModel.train_on_batch(x_batch_train, y_batch_train);\n",
        "\n",
        "        # Update training metric.\n",
        "        # train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every n batches.\n",
        "        if step % 10 == 0:\n",
        "          train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "          train_logits = myModel.predict(x_batch_train)\n",
        "          # Update val metrics\n",
        "          train_logits = np.expand_dims(train_logits.argmax(axis=1), axis=-1)\n",
        "          y_batch_train = tf.expand_dims(tf.argmax(y_batch_train, axis=1), -1)\n",
        "          train_acc_metric.update_state(train_logits, y_batch_train)\n",
        "\n",
        "          train_acc = train_acc_metric.result()\n",
        "          train_acc_metric.reset_states()\n",
        "\n",
        "          print(\n",
        "              \"   Training loss (for one batch) at step %d: %.4f\"\n",
        "              % (step, sum(loss_value) / len(loss_value))\n",
        "          )\n",
        "          print(\n",
        "              \"   Training accuracy (for one batch) at step %d: %.4f\"\n",
        "              % (step, train_acc)\n",
        "          )\n",
        "          print(\"   Seen so far: %d samples\\n\" % ((step + 1) * 32))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    # train_acc = train_acc_metric.result()\n",
        "    # print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    # train_acc_metric.reset_states()\n",
        "\n",
        "  # Run a validation loop at the end of each epoch.\n",
        "  # matric\n",
        "  val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_logits = myModel.predict(x_batch_val)\n",
        "    # Update val metrics\n",
        "    val_logits = np.expand_dims(val_logits.argmax(axis=1), axis=-1)\n",
        "    y_batch_val = tf.expand_dims(tf.argmax(y_batch_val, axis=1), -1)\n",
        "    val_acc_metric.update_state(val_logits, y_batch_val)\n",
        "\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "  print(\"data batch %d trained, time taken: %.2fs\" % (i, time.time() - start_time))\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "data batch 0 loaded\n",
            "Start of epoch 0\n",
            "tf.Tensor(47, shape=(), dtype=int8)\n",
            "Start of epoch 1\n",
            "tf.Tensor(38, shape=(), dtype=int8)\n",
            "Start of epoch 2\n",
            "tf.Tensor(-10, shape=(), dtype=int8)\n",
            "Start of epoch 3\n",
            "tf.Tensor(121, shape=(), dtype=int8)\n",
            "Start of epoch 4\n",
            "tf.Tensor(31, shape=(), dtype=int8)\n",
            "[[0.2053328  0.1942116  0.20771752 0.19980875 0.19292934]\n",
            " [0.19985715 0.19507903 0.21135288 0.20384535 0.18986557]\n",
            " [0.21137752 0.1914948  0.20919135 0.19970614 0.1882302 ]\n",
            " [0.21202397 0.1897097  0.21063684 0.19846411 0.18916535]\n",
            " [0.19362317 0.19248371 0.21326412 0.20768552 0.19294353]\n",
            " [0.2156697  0.18721834 0.20412368 0.19740318 0.1955851 ]\n",
            " [0.20404688 0.19842134 0.2076878  0.19438002 0.19546399]\n",
            " [0.21734373 0.18902615 0.20741889 0.194925   0.19128619]\n",
            " [0.19247189 0.20234276 0.21118543 0.2030482  0.19095175]\n",
            " [0.2012854  0.19881679 0.2107865  0.20021011 0.18890122]\n",
            " [0.20381203 0.19898324 0.20853454 0.19649221 0.19217795]\n",
            " [0.20880662 0.18319023 0.21649402 0.19940266 0.19210654]\n",
            " [0.19498774 0.19545707 0.21937163 0.20718864 0.18299493]\n",
            " [0.19979672 0.19450048 0.21590334 0.19898014 0.19081938]\n",
            " [0.1989309  0.1859471  0.21769828 0.19961062 0.19781308]\n",
            " [0.21078396 0.17919728 0.22022231 0.20329641 0.18649997]\n",
            " [0.20873515 0.1890768  0.21408421 0.20518339 0.18292041]\n",
            " [0.20745192 0.19291358 0.21219537 0.20295705 0.18448213]\n",
            " [0.20039925 0.20020293 0.21439953 0.20460862 0.18038963]\n",
            " [0.20062466 0.19020425 0.21482432 0.2059917  0.18835503]\n",
            " [0.21427561 0.19888039 0.20847608 0.20108946 0.17727841]\n",
            " [0.2064488  0.19014023 0.21588306 0.19995962 0.1875683 ]\n",
            " [0.21081169 0.1878062  0.20871991 0.19787051 0.19479169]\n",
            " [0.20397554 0.18919943 0.21233392 0.20260179 0.19188932]\n",
            " [0.20042606 0.19624597 0.21248733 0.20770343 0.18313722]\n",
            " [0.19651677 0.19241089 0.20998508 0.20967245 0.19141485]\n",
            " [0.20274682 0.19135822 0.21531603 0.2074317  0.18314725]\n",
            " [0.21445876 0.1875203  0.21279211 0.20102629 0.1842025 ]\n",
            " [0.20545039 0.1963963  0.20320846 0.2017236  0.19322129]\n",
            " [0.20616777 0.19295385 0.21168174 0.19962135 0.18957531]\n",
            " [0.20260704 0.19062766 0.21697228 0.20244873 0.18734434]]\n",
            "Validation acc: 0.2581\n",
            "data batch 0 trained, time taken: 1.13s\n",
            "\n",
            "\n",
            "data batch 1 loaded\n",
            "Start of epoch 0\n",
            "tf.Tensor(61, shape=(), dtype=int8)\n",
            "Start of epoch 1\n",
            "tf.Tensor(19, shape=(), dtype=int8)\n",
            "Start of epoch 2\n",
            "tf.Tensor(-21, shape=(), dtype=int8)\n",
            "Start of epoch 3\n",
            "tf.Tensor(96, shape=(), dtype=int8)\n",
            "Start of epoch 4\n",
            "tf.Tensor(95, shape=(), dtype=int8)\n",
            "[[0.20445153 0.17453447 0.21849583 0.19542873 0.20708942]\n",
            " [0.22449532 0.17962684 0.21307188 0.18233334 0.2004726 ]\n",
            " [0.20337479 0.18721555 0.21196318 0.19134298 0.2061035 ]\n",
            " [0.24469064 0.17037155 0.19730116 0.19137065 0.19626597]\n",
            " [0.22921921 0.18346745 0.2104399  0.19074705 0.18612641]\n",
            " [0.20080791 0.18813871 0.22113734 0.20049378 0.18942234]\n",
            " [0.21476366 0.18895978 0.20009832 0.20029627 0.195882  ]\n",
            " [0.22863023 0.1926191  0.2080416  0.18960588 0.18110314]\n",
            " [0.22389364 0.19409622 0.20259091 0.18705525 0.192364  ]\n",
            " [0.2043036  0.17436288 0.2223501  0.1826404  0.21634291]\n",
            " [0.20899266 0.18607189 0.22979715 0.181295   0.19384329]\n",
            " [0.20617205 0.1915232  0.21514194 0.18585187 0.201311  ]\n",
            " [0.2207335  0.18641523 0.20432238 0.19372559 0.19480333]\n",
            " [0.20944276 0.19084378 0.21179307 0.19955973 0.18836072]\n",
            " [0.20935406 0.1763297  0.22082071 0.19184703 0.20164847]\n",
            " [0.2246627  0.17529586 0.20884138 0.19892481 0.1922752 ]\n",
            " [0.2071208  0.16767272 0.21103172 0.2016449  0.21252987]\n",
            " [0.22628213 0.1966361  0.19904152 0.2010178  0.17702241]\n",
            " [0.24486332 0.1709144  0.20689003 0.1908631  0.18646914]\n",
            " [0.21169052 0.17327812 0.2198403  0.1968921  0.19829893]\n",
            " [0.22442834 0.17361392 0.21167745 0.19745207 0.19282821]\n",
            " [0.23630957 0.17345788 0.20954515 0.19055451 0.19013292]\n",
            " [0.20678782 0.1818575  0.22125493 0.19908436 0.19101547]\n",
            " [0.2138645  0.16591187 0.22422609 0.19872099 0.19727652]\n",
            " [0.17718655 0.18860206 0.22261569 0.2142049  0.19739082]\n",
            " [0.20334184 0.19449916 0.20714271 0.20197758 0.19303864]\n",
            " [0.19000447 0.19468749 0.20386793 0.20922326 0.20221691]\n",
            " [0.22548702 0.18012495 0.22195555 0.18797809 0.18445441]\n",
            " [0.22125304 0.18389265 0.21521819 0.18181066 0.19782545]\n",
            " [0.1866301  0.18394312 0.2215105  0.20366305 0.2042533 ]\n",
            " [0.20805162 0.18789801 0.22216631 0.20486185 0.17702217]]\n",
            "Validation acc: 0.4516\n",
            "data batch 1 trained, time taken: 1.02s\n",
            "\n",
            "\n",
            "data batch 2 loaded\n",
            "Start of epoch 0\n",
            "tf.Tensor(103, shape=(), dtype=int8)\n",
            "Start of epoch 1\n",
            "tf.Tensor(37, shape=(), dtype=int8)\n",
            "Start of epoch 2\n",
            "tf.Tensor(-64, shape=(), dtype=int8)\n",
            "Start of epoch 3\n",
            "tf.Tensor(-104, shape=(), dtype=int8)\n",
            "Start of epoch 4\n",
            "tf.Tensor(94, shape=(), dtype=int8)\n",
            "[[0.19054481 0.1958864  0.17664981 0.22413202 0.21278696]\n",
            " [0.16954571 0.21497935 0.19221523 0.21103418 0.21222548]\n",
            " [0.16255687 0.2362739  0.19021678 0.22030686 0.19064558]\n",
            " [0.203465   0.18598536 0.18984309 0.19623111 0.2244754 ]\n",
            " [0.22074625 0.20241931 0.18174437 0.18351366 0.21157643]\n",
            " [0.23440053 0.21991177 0.18567425 0.19436333 0.16565011]\n",
            " [0.21255158 0.20313002 0.18517156 0.19691083 0.20223604]\n",
            " [0.20272419 0.20641428 0.18464164 0.21325812 0.19296175]\n",
            " [0.17396043 0.20989522 0.19413175 0.2121845  0.20982803]\n",
            " [0.23242563 0.198927   0.19010793 0.20553765 0.17300183]\n",
            " [0.19654971 0.21023259 0.1990279  0.22994223 0.16424756]\n",
            " [0.1780335  0.21805899 0.19948688 0.20558436 0.19883625]\n",
            " [0.2001536  0.20939411 0.19092564 0.2107838  0.18874289]\n",
            " [0.20570697 0.21278028 0.1918743  0.19592917 0.19370927]\n",
            " [0.17273276 0.2178766  0.20663954 0.21228242 0.1904687 ]\n",
            " [0.1880846  0.22503348 0.19196609 0.20043634 0.19447953]\n",
            " [0.18648456 0.22021039 0.18813336 0.19814163 0.20703003]\n",
            " [0.18108422 0.21275093 0.20668055 0.20185226 0.19763204]\n",
            " [0.2029594  0.22268115 0.1910472  0.20998316 0.17332906]\n",
            " [0.21037681 0.20657039 0.1917128  0.21175861 0.1795814 ]\n",
            " [0.20025653 0.21285003 0.1829462  0.2016679  0.20227931]\n",
            " [0.19218072 0.21879098 0.20329382 0.20835307 0.17738149]\n",
            " [0.22580849 0.19683585 0.18806787 0.19762242 0.19166537]\n",
            " [0.19609697 0.22105268 0.19849204 0.1882734  0.19608492]\n",
            " [0.22280315 0.19142489 0.1883557  0.21054822 0.18686809]\n",
            " [0.19285773 0.21610521 0.177082   0.19701642 0.21693863]\n",
            " [0.19978279 0.2045469  0.19215155 0.19941376 0.20410505]\n",
            " [0.16906956 0.21674828 0.19132659 0.21727651 0.205579  ]\n",
            " [0.20517544 0.20662189 0.1944691  0.20824565 0.18548793]\n",
            " [0.18742155 0.20316033 0.19437812 0.2064879  0.20855206]\n",
            " [0.16326825 0.2038991  0.19673137 0.2265659  0.20953535]]\n",
            "Validation acc: 0.1935\n",
            "data batch 2 trained, time taken: 1.04s\n",
            "\n",
            "\n",
            "data batch 3 loaded\n",
            "Start of epoch 0\n",
            "tf.Tensor(12, shape=(), dtype=int8)\n",
            "Start of epoch 1\n",
            "tf.Tensor(90, shape=(), dtype=int8)\n",
            "Start of epoch 2\n",
            "tf.Tensor(-78, shape=(), dtype=int8)\n",
            "Start of epoch 3\n",
            "tf.Tensor(-105, shape=(), dtype=int8)\n",
            "Start of epoch 4\n",
            "tf.Tensor(46, shape=(), dtype=int8)\n",
            "[[0.15187496 0.16541983 0.17780697 0.21773145 0.28716677]\n",
            " [0.2296487  0.20441814 0.17664535 0.2436817  0.14560613]\n",
            " [0.17276725 0.2039505  0.14627704 0.21694142 0.26006383]\n",
            " [0.14046977 0.12324688 0.18427838 0.20131359 0.35069135]\n",
            " [0.14358903 0.13915536 0.15986384 0.20681684 0.3505749 ]\n",
            " [0.17367977 0.19111057 0.17673145 0.24262913 0.21584904]\n",
            " [0.1502654  0.2056971  0.17699277 0.23297091 0.23407383]\n",
            " [0.13658302 0.15534945 0.1870102  0.24855678 0.27250051]\n",
            " [0.16558228 0.18582663 0.16740347 0.2350075  0.24618015]\n",
            " [0.18774438 0.19863999 0.17517105 0.20782429 0.23062028]\n",
            " [0.1607254  0.19956556 0.18366967 0.23956051 0.21647893]\n",
            " [0.1570276  0.22161943 0.1519373  0.22686493 0.2425507 ]\n",
            " [0.16710688 0.15429674 0.21238378 0.22285618 0.24335638]\n",
            " [0.15420146 0.1826073  0.19073619 0.19817051 0.2742845 ]\n",
            " [0.20861061 0.14250934 0.18956624 0.22386448 0.23544931]\n",
            " [0.12417869 0.21295694 0.14194387 0.24648918 0.2744313 ]\n",
            " [0.15876712 0.12343381 0.16388546 0.23922053 0.3146931 ]\n",
            " [0.17506638 0.16294041 0.1923829  0.20786893 0.2617414 ]\n",
            " [0.1457892  0.13835587 0.17474623 0.20447774 0.3366309 ]\n",
            " [0.13676085 0.16942386 0.15621428 0.2100298  0.3275712 ]\n",
            " [0.1433825  0.18180223 0.17816377 0.2201143  0.27653718]\n",
            " [0.16580404 0.2042545  0.16255067 0.23669033 0.23070046]\n",
            " [0.1399636  0.15147704 0.19273663 0.21002583 0.30579692]\n",
            " [0.1307897  0.19491847 0.1869691  0.24560392 0.24171875]\n",
            " [0.14362802 0.19745384 0.15453045 0.23065534 0.27373233]\n",
            " [0.15352614 0.15452869 0.18382387 0.20106287 0.30705842]\n",
            " [0.13097613 0.1309956  0.16593571 0.21053174 0.36156082]\n",
            " [0.16173546 0.15715407 0.17363563 0.16886908 0.33860576]\n",
            " [0.14346918 0.11523255 0.17538002 0.2009888  0.36492944]\n",
            " [0.14252505 0.21790399 0.15349638 0.18617046 0.29990417]\n",
            " [0.16352466 0.12885422 0.16524138 0.20425354 0.3381262 ]]\n",
            "Validation acc: 0.0000\n",
            "data batch 3 trained, time taken: 1.02s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-df7877e0f273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmyData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDR_resized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\ndata batch %d loaded\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-301b9d593462>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, is_training, batch_id, batch_size, train_val_split_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;31m# load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mall_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Processed_Data_Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Even_Class_Distribution_Datasets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Processed_Xtrain_Batch%d_even.npy\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;31m# all_images = np.load(os.path.join(project_root, \"Processed_Data_Batches\", \"Stratified_Random_Sampling_Datasets\", \"Processed_Xtrain_Batch%d.npy\" % batch_id))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# add a dimension for channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0svWpVPkiUsK"
      },
      "source": [
        "# testing process\n",
        "file_num = 2\n",
        "num_list = []\n",
        "acc_list = []\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  num, acc = myModel.evaluate(myData.test_images, myData.test_labels)\n",
        "  print(num, acc)\n",
        "  num_list.append(num)\n",
        "  acc_list.append(acc)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL9za1Dkqjy"
      },
      "source": [
        "# compute overall accuracy\n",
        "num_list = np.array(num_list)\n",
        "acc_list = np.array(acc_list)\n",
        "acc_overall = (num_list * acc_list).sum() / num_list.sum()\n",
        "print(acc_overall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBuQpdk7vC25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11329b10-2bf3-4f70-e080-94b9837fa877"
      },
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(\"date and time =\", dt_string)\n",
        "myModel.save_model(os.path.join(project_root, \"Trained Models\", dt_string))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date and time = 23-03-2021 19:18:56\n",
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/CS 766 Project/Project Coding and Data Files/Trained Models/23-03-2021 19:18:56/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2QDEecGwGTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475aaddc-98d7-4ce9-bb79-1748bf08337a"
      },
      "source": [
        "# this part is for loading parameters and extracting features\n",
        "# load parameters\n",
        "parameter_version = \"23-03-2021 19:18:56\"\n",
        "if not os.path.exists(os.path.join(project_root, \"features\", parameter_version)):\n",
        "    os.mkdir(os.path.join(project_root, \"features\", parameter_version))\n",
        "\n",
        "my_model_pretrained = OriginCNN()\n",
        "my_model_pretrained.build()\n",
        "my_model_pretrained.load_model(os.path.join(project_root, \"Trained Models\", parameter_version))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 333, 333, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 166, 166, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 55, 55, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 27, 27, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 35,397\n",
            "Trainable params: 35,397\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dC4KT0HjTqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e7e0fa-f530-4152-edf2-bf191de684df"
      },
      "source": [
        "# load training data, extract features and save to npy files\n",
        "file_num = 9\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(True, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.train_images)\n",
        "  # feature = myModel.extract_feature(myData.train_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtrain_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtrain_feature%d.npy\" % i)\n",
        "  print(feature)\n",
        "  myData.clear()\n",
        "  gc.collect()\n",
        "\n",
        "# load testing data, extract features and save to npy files\n",
        "file_num = 2\n",
        "for i in range(file_num):\n",
        "  myData = DR_resized(False, i, 32, 0.1)\n",
        "  feature = my_model_pretrained.extract_feature(myData.test_images)\n",
        "  np.save(os.path.join(project_root, \"features\", parameter_version, \"Xtest_feature%d.npy\" % i), feature)\n",
        "  print(\"Xtest_feature%d.npy\" % i)\n",
        "  myData.clear()\n",
        "  gc.collect()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain_feature0.npy\n",
            "[[0.35830727 0.10113482 0.         ... 0.         0.         0.47119415]\n",
            " [0.35830727 0.04669811 0.         ... 0.         0.         0.200975  ]\n",
            " [0.47821516 0.11293279 0.         ... 0.         0.         0.31759244]\n",
            " ...\n",
            " [0.35830727 0.10821177 0.         ... 0.         0.         0.3621609 ]\n",
            " [0.38736483 0.         0.         ... 0.         0.         0.50423074]\n",
            " [0.35830727 0.12817381 0.         ... 0.         0.         0.16432361]]\n",
            "Xtrain_feature1.npy\n",
            "[[0.31001762 0.08447442 0.         ... 0.         0.         0.54034066]\n",
            " [0.45026395 0.04669811 0.         ... 0.         0.         0.62844735]\n",
            " [0.36011726 0.10609142 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.39272144 0.1103986  0.         ... 0.         0.         0.20643379]\n",
            " [0.37816614 0.12638164 0.         ... 0.         0.         0.36818033]\n",
            " [0.48419    0.46840778 0.         ... 0.         0.         1.2397174 ]]\n",
            "Xtrain_feature2.npy\n",
            "[[0.5399451  0.04669811 0.         ... 0.         0.         0.33048525]\n",
            " [0.60250944 0.18666117 0.         ... 0.         0.         0.40565366]\n",
            " [0.27334434 0.14828078 0.         ... 0.         0.         0.452514  ]\n",
            " ...\n",
            " [0.5731983  0.04669811 0.         ... 0.         0.         0.09627229]\n",
            " [0.47941777 0.47430038 0.         ... 0.         0.         0.23661497]\n",
            " [0.37783462 0.09423186 0.         ... 0.         0.         0.24922517]]\n",
            "Xtrain_feature3.npy\n",
            "[[0.32879612 0.         0.         ... 0.         0.         0.4694653 ]\n",
            " [0.6938508  0.05547374 0.         ... 0.         0.         0.91222274]\n",
            " [0.36964494 0.19902071 0.         ... 0.         0.         0.71168756]\n",
            " ...\n",
            " [0.4383259  0.15725595 0.         ... 0.         0.         0.4951031 ]\n",
            " [0.5663408  0.04669811 0.         ... 0.         0.         0.59657353]\n",
            " [0.3189979  0.26821998 0.02016739 ... 0.         0.         0.293995  ]]\n",
            "Xtrain_feature4.npy\n",
            "[[0.5482393  0.08710946 0.         ... 0.         0.         0.5158137 ]\n",
            " [0.35830727 0.09960653 0.         ... 0.         0.         0.54345   ]\n",
            " [0.47943452 0.12549722 0.         ... 0.         0.         0.5850111 ]\n",
            " ...\n",
            " [0.31712344 0.37991413 0.         ... 0.         0.         0.47146785]\n",
            " [0.3831841  0.08544667 0.         ... 0.         0.         0.7459256 ]\n",
            " [0.4939859  0.07166357 0.         ... 0.         0.         0.        ]]\n",
            "Xtrain_feature5.npy\n",
            "[[0.25912037 0.10348704 0.         ... 0.         0.         0.2797989 ]\n",
            " [0.56932783 0.053529   0.         ... 0.         0.         0.7505858 ]\n",
            " [0.34870118 0.04483921 0.         ... 0.         0.         0.547003  ]\n",
            " ...\n",
            " [0.50984347 0.11558133 0.         ... 0.         0.         0.43577933]\n",
            " [0.35830727 0.04669811 0.         ... 0.         0.         0.35837883]\n",
            " [0.35321733 0.04388131 0.         ... 0.         0.         0.63443315]]\n",
            "Xtrain_feature6.npy\n",
            "[[0.3554677  0.2251327  0.         ... 0.         0.         1.0805299 ]\n",
            " [0.34170717 0.10629591 0.         ... 0.         0.         0.16824785]\n",
            " [0.3612893  0.10474242 0.         ... 0.         0.         0.7805975 ]\n",
            " ...\n",
            " [0.5219863  0.22546814 0.         ... 0.         0.         0.7206606 ]\n",
            " [0.52372545 0.19352484 0.         ... 0.         0.         1.0042638 ]\n",
            " [0.47989467 0.11977142 0.         ... 0.         0.         0.89010364]]\n",
            "Xtrain_feature7.npy\n",
            "[[0.46209344 0.04669811 0.         ... 0.         0.         1.5074818 ]\n",
            " [0.35830727 0.14953604 0.         ... 0.         0.         0.7582077 ]\n",
            " [0.6095504  0.09464127 0.         ... 0.         0.         0.19994576]\n",
            " ...\n",
            " [0.8160008  0.06942797 0.         ... 0.         0.         0.40248483]\n",
            " [0.37412393 0.448774   0.         ... 0.         0.         0.27251655]\n",
            " [0.3598394  0.10610101 0.         ... 0.         0.         0.98659545]]\n",
            "Xtrain_feature8.npy\n",
            "[[0.39482647 0.11724132 0.         ... 0.         0.         0.4347384 ]\n",
            " [0.39845127 0.4381626  0.         ... 0.         0.         0.48718914]\n",
            " [0.63168937 0.25750208 0.         ... 0.         0.         1.0495282 ]\n",
            " ...\n",
            " [0.6317685  0.13022763 0.         ... 0.         0.         0.38409802]\n",
            " [0.59687954 0.17956656 0.         ... 0.         0.         1.5910499 ]\n",
            " [0.28515217 0.11274222 0.         ... 0.         0.         0.6627514 ]]\n",
            "Xtest_feature0.npy\n",
            "Xtest_feature1.npy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}